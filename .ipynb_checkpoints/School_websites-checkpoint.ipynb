{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Text from School Websites\n",
    "\n",
    "<br>\n",
    "In this ongoing project, I explore how different UK schools choose to present themselves on their websites. Do schools with poor academic performance emphasize other attributes, and if so which? Do different types of schools - such as academies, community schools, and private schools - choose to highlight different characteristics? How do the characteristics that schools choose to emphasize correspond to measures that we have of underlying realities? \n",
    "\n",
    "The primary goal of this project is to help parents choosing between schools to evaluate and make use of the marketing with which the are prsented. I will investigate the extent to which parents should trust what schools say; for example, do the attributes that schools choose to emphasize correlate with the administrative measures or the perceptions of parents with children at a school? I will also explore whether there are any useful heuristics that parents can employ in examining school reviews; for example, if a school fails to mention academic performance, is this a bad sign?\n",
    "\n",
    "I begin by discussing how I constructed the dataset of text from school webisites used in this analysis. After loading the packages and text data used in this analysis, I then employ two different approachs to analyzing the text from school websites: clustering with k-means and mixed membership modeling with Latent Dirichlet Allocation (LDA). I then link the output from these analyses of my text data to datasets on school performance and other features, and explore the relationship between the text from school websites and other school characteristics.\n",
    "\n",
    "This project is very much a work in progress, and throughout I will point to text steps that I plan to take in my analysis.\n",
    "\n",
    "# Contents\n",
    "\n",
    "<ul>\n",
    "<li>[Import packages](#Import-packages)\n",
    "<li>[Construct dataset from school websites](#Construct-dataset-from-school-websites)\n",
    "<li>[Load school website data](#Load-school-website-data)\n",
    "<li>[K-means](#K-means)\n",
    "<li>[Latent dirichlet allocation (LDA)](#Latent-dirichlet-allocation-(LDA)\n",
    "<li>[Combine datasets](#Merging-datasets)\n",
    "<li>[Analyze relationship between website text and school characteristics](#Analyze-relationship-between-website-text-and-school-characteristics)\n",
    "</ul>\n",
    "\n",
    "# Import packages & load blurbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import gensim\n",
    "import json\n",
    "import random\n",
    "import webbrowser\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize                  \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora, models\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct dataset from school websites\n",
    "\n",
    "I investigate how schools present themselves on their websites by analyzing text from the homepages of school websites. Specifically, I focus on the introductory \"blurb\" found on many school websites, for example:\n",
    "\n",
    "<img src=\"blurb_example.png\">\n",
    "\n",
    "I began by downloading a list of 26,641 urls for the websites of schools in England from the Department of Education's [Edubase public portal](http://www.education.gov.uk/edubase/home.xhtml). I then scraped the text from all of the working URLs using [Scrapy](https://scrapy.org/). \n",
    "\n",
    "The primary difficulty in building my scraper was that schools have text about all kinds of things on their homepage, and comparing, for example, the introductory blurb from one website to a pile of news from another is unlikely to yield much interesting. Further complicating matters, the construction of websites is inconsistent across schools, making it difficult to easily capture the introductory blurb.\n",
    "\n",
    "I began by taking the largest block of text that I could find on each page, but this missed a lot of blurbs I wanted to catch, only caught part of some blurbs, and picked up a lot of other mess. However, I noticed that many blurbs either started with \"Welcome\", or ended with the headteacher signing off.\n",
    "\n",
    "I therefore wrote a spider that, in short, would collect paragraphs of text below \"Welcome\" or above \"Headteacher\", \"Head of School\", or various other words used for principal. I limited the blurbs I scraped to strings of 200 words or greater.\n",
    "\n",
    "This scraper produced a databse of X text reviews.\n",
    "\n",
    "## Exploratory analysis of sample\n",
    "\n",
    "It is worth first understanding a bit about this sample. Of X institutions listed in the EduBase datset, X have urls in the dataset, and of these I have obtained blurbs for X schools.\n",
    "\n",
    "First, I take a look at the proportion of observations for which I have urls, was woring urls, was able to scrape some text etc.\n",
    "\n",
    "***To load the dataset, I had to delete the observation for http://www.stmh.co.uk/. It had filled up with no blurb, and instead listing a bunch of column headings. I'm not sure why - when I tried running the scraper for just that observation it worked fine.***\n",
    "\n",
    "***I should merge datasets early, and drop obs for which the establishment is closed***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schools in EduBase dataset: 45143\n",
      "Obs before scraping (obs for which have URLs): 26641\n",
      "Obs after scraping: 26641\n",
      "\n",
      "Looking at obs after scraping....\n",
      "\n",
      "URL worked but no blurb: 5388\n",
      "\n",
      "Total errors: 2825\n",
      "Other error: 675\n",
      "HTTP error: 534\n",
      "DNS error: 1597\n",
      "Timeout error: 19\n",
      "\n",
      "Total found blurb: 18428\n",
      "Blurbs >4k characters: 37\n",
      "Blurbs <250 characters: 12861\n",
      "Blurbs >=250 & <=4k characters: 5530\n",
      "\n",
      "Check totals sum correctly: 26641\n"
     ]
    }
   ],
   "source": [
    "edubase_dat = pd.read_csv('edubasealldata.csv', low_memory=False)\n",
    "url_dat = pd.read_csv('Scraping/csvurls.csv', names=['URN','URL'])  ##urls after dropping nulls\n",
    "blurbs_dat = pd.read_csv('Scraping/blurbs_spider/blurbs_dat.csv')\n",
    "\n",
    "print 'Schools in EduBase dataset:', len(edubase_dat)\n",
    "print 'Obs before scraping (obs for which have URLs):', len(url_dat)\n",
    "print 'Obs after scraping:', len(blurbs_dat)\n",
    "print\n",
    "print 'Looking at obs after scraping....'\n",
    "print \n",
    "print 'URL worked but no blurb:', sum(blurbs_dat['flag']=='Found_url_but_no_blurb')\n",
    "print\n",
    "total_errors = sum(blurbs_dat['flag']=='Other_error') + sum(blurbs_dat['flag']=='HTTP_error') + \\\n",
    "        sum(blurbs_dat['flag']=='DNS_error') + sum(blurbs_dat['flag']=='Timeout_error')\n",
    "print 'Total errors:', total_errors\n",
    "print 'Other error:', sum(blurbs_dat['flag']=='Other_error')\n",
    "print 'HTTP error:', sum(blurbs_dat['flag']=='HTTP_error')\n",
    "print 'DNS error:', sum(blurbs_dat['flag']=='DNS_error')\n",
    "print 'Timeout error:', sum(blurbs_dat['flag']=='Timeout_error')\n",
    "print\n",
    "\n",
    "found_blurb = blurbs_dat[blurbs_dat['flag']=='Found_blurb']\n",
    "print 'Total found blurb:', len(found_blurb) \n",
    "# Many of the blurbs of more than 4000 characters in length appear to have picked up text I wasn't\n",
    "# aiming for, such as separate blocks of text on school news. Under 4000, in contrast, the blurbs\n",
    "# I examined appear to be just legitimately long blurbs. I will therefore drop the 15 blurbs that \n",
    "# are longer than 4000 characters\n",
    "# print sorted(list(found_blurb['length']), reverse=True)[:100]\n",
    "# for x in sorted(list(found_blurb['length']), reverse=True)[:20]:\n",
    "#     print x\n",
    "#     print list(found_blurb.loc[found_blurb['length']==x, 'blurb'])  \n",
    "\n",
    "# Likewise, it is only when we reach 200-250 characters that the text starts to represent a meaningful\n",
    "# welcome blurb with any informatino about the school. Below this, I either captured only part of the \n",
    "# blurb (for example, in many case I just picked up \"Welcome\") or, in most cases, the blurb just said \n",
    "# welcome to the webiste and gave little additional information. I will therefore limit the sample to \n",
    "# blurbs of 250 or more characters.\n",
    "# print list(dat.loc[(dat['length']<250) & (dat['length']>240), 'blurb'])\n",
    "\n",
    "print 'Blurbs >4k characters:', len(found_blurb[found_blurb['length']>4000])  \n",
    "print 'Blurbs <250 characters:', len(found_blurb[found_blurb['length']<250])  \n",
    "print 'Blurbs >=250 & <=4k characters:', len(found_blurb[(found_blurb['length']>=250) & (found_blurb['length']<=4000)])\n",
    "print\n",
    "\n",
    "print 'Check totals sum correctly:', sum(dat['flag']=='Found_url_but_no_blurb') + total_errors + len(found_blurb) \n",
    "\n",
    "## Delete unneeded datsets to free up space\n",
    "del found_blurb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I next restrict my sample to all open state-funded and independent schools. I exclude closed establishments, as well as other types of establishment such as high education institutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EduBase dataset contains no duplicate URNs: True\n",
      "Database after scraping contains no duplicate URNs: True\n",
      "State datset contains no dublicate URNs: True\n",
      "Obs in state funded schools dataset: 21917\n",
      "Obs after limiting edubase to establishments in state funded schools dataset: 21917\n",
      "Reduced Edubase dataset and state dataset use same school types: True\n",
      "Number of school types: 22\n",
      "\n",
      "Academy 16-19 Converter\n",
      "Academy 16-19 Sponsor Led\n",
      "Academy Alternative Provision Converter\n",
      "Academy Alternative Provision Sponsor Led\n",
      "Academy Converter\n",
      "Academy Special Converter\n",
      "Academy Special Sponsor Led\n",
      "Academy Sponsor Led\n",
      "Community School\n",
      "Community Special School\n",
      "Foundation School\n",
      "Foundation Special School\n",
      "Free Schools\n",
      "Free Schools - 16-19\n",
      "Free Schools - Alternative Provision\n",
      "Free Schools Special\n",
      "LA Nursery School\n",
      "Pupil Referral Unit\n",
      "Studio Schools\n",
      "University Technical College\n",
      "Voluntary Aided School\n",
      "Voluntary Controlled School\n",
      "\n",
      "British Schools Overseas\n",
      "City Technology College\n",
      "Further Education\n",
      "Higher Education Institutions\n",
      "Institution funded by other Government Department\n",
      "Miscellaneous\n",
      "Non-Maintained Special School\n",
      "Offshore Schools\n",
      "Other Independent School\n",
      "Other Independent Special School\n",
      "Secure Units\n",
      "Service Childrens Education\n",
      "Sixth Form Centres\n",
      "Special Post 16 Institution\n",
      "Welsh Establishment\n",
      "\n",
      "State schools obs: 21917\n",
      "Indepdent schools obs: 2306\n",
      "Obs in state and indep dataset equals totals: True\n",
      "Obs in state and indep dataset: 24223\n",
      "\n",
      "Schools in EduBase dataset: 24223\n",
      "Obs for which have URLs: 20771\n",
      "Obs after scraping: 20771\n",
      "\n",
      "Looking at obs after scraping....\n",
      "URL worked but no blurb: 4160\n",
      "\n",
      "Total errors: 814\n",
      "Other error: 173\n",
      "HTTP error: 238\n",
      "DNS error: 395\n",
      "Timeout error: 8\n",
      "\n",
      "Total found blurb: 15797\n",
      "Blurbs >4k characters: 29\n",
      "Blurbs <250 characters: 11018\n",
      "Blurbs >=250 & <=4k characters: 4750\n"
     ]
    }
   ],
   "source": [
    "## Take a look at column names in edubase\n",
    "# print 'Column names:'\n",
    "# for col_name in list(edubase_dat.columns.values):\n",
    "#     print col_name\n",
    "# print \n",
    "\n",
    "## Select key variables from edubase\n",
    "edubase = edubase_dat.copy()\n",
    "edubase = edubase[['URN', 'SchoolWebsite', 'TypeOfEstablishment (name)', 'EstablishmentStatus (name)']]\n",
    "\n",
    "## Rename columns\n",
    "edubase.columns = ['TypeOfEstablishment' if x=='TypeOfEstablishment (name)' else x for x in edubase.columns]\n",
    "edubase.columns = ['EstablishmentStatus' if x=='EstablishmentStatus (name)' else x for x in edubase.columns]\n",
    "\n",
    "## Check that URN is a unique identifier in edubase and blurbs datasets\n",
    "print 'EduBase dataset contains no duplicate URNs:', len(edubase['URN'].unique())== len(edubase)\n",
    "print 'Database after scraping contains no duplicate URNs:', len(dat['urn'].unique()) == len(dat)\n",
    "\n",
    "## Load database of open state funded schools and get list of URNS\n",
    "state = pd.read_csv('Edubase_datasets/edubaseallstatefunded20170218.csv', low_memory=False)\n",
    "state = state[['URN', 'TypeOfEstablishment (name)', 'EstablishmentStatus (name)']]\n",
    "state.columns = ['URN', 'State_df_type', 'State_df_status']\n",
    "print 'State datset contains no dublicate URNs:', len(state['URN'].unique())== len(state)\n",
    "print 'Obs in state funded schools dataset:', len(state)\n",
    "\n",
    "## Select obs from edubase where URN in state funded open state funded schools database\n",
    "## and take a look at school types included in state and new datasets\n",
    "ed_state = pd.merge(state, edubase, how='left', on='URN')\n",
    "print 'Obs after limiting edubase to establishments in state funded schools dataset:', len(ed_state)\n",
    "\n",
    "## Confirm that edbuase and state datsets use same coding for school type\n",
    "print 'Reduced Edubase dataset and state dataset use same school types:', \\\n",
    "len(sorted(list(ed_state['TypeOfEstablishment'].unique()))) == \\\n",
    "len(sorted(list(state['State_df_type'].unique())))\n",
    "del ed_state['State_df_type']\n",
    "del ed_state['State_df_status']\n",
    "\n",
    "## Take a look at the school types in my dataset of open state funded schools\n",
    "print 'Number of school types:', len(sorted(list(ed_state['TypeOfEstablishment'].unique())))\n",
    "print\n",
    "for school_type in sorted(list(ed_state['TypeOfEstablishment'].unique())):\n",
    "    print school_type\n",
    "print\n",
    "\n",
    "## Select obs where URN in state funded open state funded schools database\n",
    "## and take a look at school types included\n",
    "ed_other = edubase[~edubase.TypeOfEstablishment.isin(list(ed_state['TypeOfEstablishment'].unique()))]\n",
    "for school_type in sorted(list(ed_other['TypeOfEstablishment'].unique())):\n",
    "    print school_type\n",
    "print\n",
    "    \n",
    "## Select independent schools from edubased dataset (must be a nice way of coding this)\n",
    "ed_ind = edubase[((edubase['TypeOfEstablishment']=='Other Independent School') |\\\n",
    "(edubase['TypeOfEstablishment']=='Other Independent Special School')) &\\\n",
    "((edubase['EstablishmentStatus']=='Open')|\\\n",
    "(edubase['EstablishmentStatus']=='Open, but proposed to close'))]\n",
    "\n",
    "## Bind dataset of open state school with open independent schools\n",
    "ed_schools = pd.concat([ed_state, ed_ind], ignore_index=True)\n",
    "print 'State schools obs:', len(ed_state)\n",
    "print 'Indepdent schools obs:', len(ed_ind)\n",
    "print 'Obs in state and indep dataset equals totals:', len(ed_schools) == len(ed_state) + len(ed_ind)\n",
    "print 'Obs in state and indep dataset:', len(ed_schools)\n",
    "print \n",
    "\n",
    "# ## Join edubase and blurbs datasets (being sure not to drop any obs)\n",
    "blurbs_dat['scraped_dataset'] = True\n",
    "df = pd.merge(ed_schools, blurbs_dat, how='left', left_on='URN', right_on='urn')\n",
    "\n",
    "## Re-run scraping summary statistics on dataset of open state schools and open \n",
    "## independent schools\n",
    "def scrapy_stats(ed_schools, df):\n",
    "    print 'Schools in EduBase dataset:', len(ed_schools)\n",
    "    print 'Obs for which have URLs:', sum(ed_schools.SchoolWebsite.notnull())\n",
    "    print 'Obs after scraping:', sum(df['scraped_dataset']==True)\n",
    "    print\n",
    "    print 'Looking at obs after scraping....'\n",
    "    print 'URL worked but no blurb:', sum(df['flag']=='Found_url_but_no_blurb')\n",
    "    print \n",
    "    total_errors = sum(df['flag']=='Other_error') + sum(df['flag']=='HTTP_error') + \\\n",
    "            sum(df['flag']=='DNS_error') + sum(df['flag']=='Timeout_error')\n",
    "    print 'Total errors:', total_errors\n",
    "    print 'Other error:', sum(df['flag']=='Other_error')\n",
    "    print 'HTTP error:', sum(df['flag']=='HTTP_error')\n",
    "    print 'DNS error:', sum(df['flag']=='DNS_error')\n",
    "    print 'Timeout error:', sum(df['flag']=='Timeout_error')\n",
    "    print \n",
    "    df_found_blurb = df[df['flag']=='Found_blurb']\n",
    "    print 'Total found blurb:', len(df_found_blurb) \n",
    "    print 'Blurbs >4k characters:', len(df_found_blurb[df_found_blurb['length']>4000])  \n",
    "    print 'Blurbs <250 characters:', len(df_found_blurb[df_found_blurb['length']<250])  \n",
    "    print 'Blurbs >=250 & <=4k characters:', len(df_found_blurb[(df_found_blurb['length']>=250) & \\\n",
    "                                                                (df_found_blurb['length']<=4000)])\n",
    "scrapy_stats(ed_schools, df)\n",
    "\n",
    "# # Create histogram of observations I will not be dropping\n",
    "# # (old code - but could be worth revising)\n",
    "# dat_red = dat[dat['length']<=4000]\n",
    "# dat_red = dat_red[dat_red['length']>=250]\n",
    "\n",
    "# plt.hist(list(dat_red['length']), bins=100)\n",
    "# plt.show()\n",
    "\n",
    "# dat_small = dat[dat['length']<=250]\n",
    "# plt.hist(list(dat_small['length']), bins=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to explore how the proportion of blurbs I have collected varies across different school types. However, the database currently includes 24 different school types. I first, therefore, create a new variable that groups school types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sch_type</th>\n",
       "      <th>edubase_tot</th>\n",
       "      <th>url_tot</th>\n",
       "      <th>scrape_tot</th>\n",
       "      <th>url_perc</th>\n",
       "      <th>blurb_tot</th>\n",
       "      <th>blurb_perc</th>\n",
       "      <th>blurb_perc_of_urls</th>\n",
       "      <th>med_tot</th>\n",
       "      <th>med_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Academy</td>\n",
       "      <td>6033</td>\n",
       "      <td>5480</td>\n",
       "      <td>5480</td>\n",
       "      <td>0.908337</td>\n",
       "      <td>4246</td>\n",
       "      <td>0.703796</td>\n",
       "      <td>0.774818</td>\n",
       "      <td>1461</td>\n",
       "      <td>0.242168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Community</td>\n",
       "      <td>8438</td>\n",
       "      <td>7863</td>\n",
       "      <td>7863</td>\n",
       "      <td>0.931856</td>\n",
       "      <td>6030</td>\n",
       "      <td>0.714624</td>\n",
       "      <td>0.766883</td>\n",
       "      <td>1676</td>\n",
       "      <td>0.198625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>973</td>\n",
       "      <td>919</td>\n",
       "      <td>919</td>\n",
       "      <td>0.944502</td>\n",
       "      <td>676</td>\n",
       "      <td>0.694758</td>\n",
       "      <td>0.735582</td>\n",
       "      <td>194</td>\n",
       "      <td>0.199383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Free</td>\n",
       "      <td>345</td>\n",
       "      <td>245</td>\n",
       "      <td>245</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>192</td>\n",
       "      <td>0.556522</td>\n",
       "      <td>0.783673</td>\n",
       "      <td>60</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Independent</td>\n",
       "      <td>2306</td>\n",
       "      <td>839</td>\n",
       "      <td>839</td>\n",
       "      <td>0.363833</td>\n",
       "      <td>538</td>\n",
       "      <td>0.233304</td>\n",
       "      <td>0.641240</td>\n",
       "      <td>173</td>\n",
       "      <td>0.075022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LA Nursery School</td>\n",
       "      <td>402</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>0.490050</td>\n",
       "      <td>129</td>\n",
       "      <td>0.320896</td>\n",
       "      <td>0.654822</td>\n",
       "      <td>38</td>\n",
       "      <td>0.094527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pupil Referral Unit</td>\n",
       "      <td>258</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>0.375969</td>\n",
       "      <td>66</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>19</td>\n",
       "      <td>0.073643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Studio Schools</td>\n",
       "      <td>36</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>20</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>8</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>University Technical College</td>\n",
       "      <td>48</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>29</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>10</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voluntary Aided School</td>\n",
       "      <td>3294</td>\n",
       "      <td>3075</td>\n",
       "      <td>3075</td>\n",
       "      <td>0.933515</td>\n",
       "      <td>2376</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.772683</td>\n",
       "      <td>681</td>\n",
       "      <td>0.206740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Voluntary Controlled School</td>\n",
       "      <td>2090</td>\n",
       "      <td>1994</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.954067</td>\n",
       "      <td>1495</td>\n",
       "      <td>0.715311</td>\n",
       "      <td>0.749749</td>\n",
       "      <td>430</td>\n",
       "      <td>0.205742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        sch_type  edubase_tot  url_tot  scrape_tot  url_perc  \\\n",
       "0                        Academy         6033     5480        5480  0.908337   \n",
       "1                      Community         8438     7863        7863  0.931856   \n",
       "2                     Foundation          973      919         919  0.944502   \n",
       "3                           Free          345      245         245  0.710145   \n",
       "4                    Independent         2306      839         839  0.363833   \n",
       "5              LA Nursery School          402      197         197  0.490050   \n",
       "6            Pupil Referral Unit          258       97          97  0.375969   \n",
       "7                 Studio Schools           36       26          26  0.722222   \n",
       "8   University Technical College           48       36          36  0.750000   \n",
       "9         Voluntary Aided School         3294     3075        3075  0.933515   \n",
       "10   Voluntary Controlled School         2090     1994        1994  0.954067   \n",
       "\n",
       "    blurb_tot  blurb_perc  blurb_perc_of_urls  med_tot  med_perc  \n",
       "0        4246    0.703796            0.774818     1461  0.242168  \n",
       "1        6030    0.714624            0.766883     1676  0.198625  \n",
       "2         676    0.694758            0.735582      194  0.199383  \n",
       "3         192    0.556522            0.783673       60  0.173913  \n",
       "4         538    0.233304            0.641240      173  0.075022  \n",
       "5         129    0.320896            0.654822       38  0.094527  \n",
       "6          66    0.255814            0.680412       19  0.073643  \n",
       "7          20    0.555556            0.769231        8  0.222222  \n",
       "8          29    0.604167            0.805556       10  0.208333  \n",
       "9        2376    0.721311            0.772683      681  0.206740  \n",
       "10       1495    0.715311            0.749749      430  0.205742  "
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for s in sorted(list(df['TypeOfEstablishment'].unique())):\n",
    "#     print s\n",
    "# print\n",
    "\n",
    "def collapse_types(df):\n",
    "    df['school_type'] = df['TypeOfEstablishment']\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:7]=='Academy', 'school_type'] ='Academy'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:9]=='Community', 'school_type'] ='Community'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:10]=='Foundation', 'school_type'] ='Foundation'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:4]=='Free', 'school_type'] ='Free'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:5]=='Other', 'school_type'] ='Independent'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:9]=='Other', 'school_type'] ='Voluntary'\n",
    "    return df\n",
    "df = collapse_types(df)\n",
    "    \n",
    "sch_type = []\n",
    "edubase_tot = []\n",
    "url_tot = []\n",
    "scrape_tot = []\n",
    "url_perc = []\n",
    "blurb_tot = []\n",
    "blurb_perc = []\n",
    "blurb_perc_of_urls = []\n",
    "med_tot = []\n",
    "med_perc = []\n",
    "for s in sorted(list(df['school_type'].unique())):\n",
    "    sub = df[df.school_type==s]\n",
    "    sch_type.append(s)\n",
    "    edubase_tot.append(len(sub))\n",
    "    url_tot.append(sum(sub.SchoolWebsite.notnull()))\n",
    "    scrape_tot.append(sum(sub['scraped_dataset']==True))\n",
    "    url_perc.append(float(sum(sub['scraped_dataset']==True))/float(len(sub)))\n",
    "    blurb_tot.append(sum(sub['flag']=='Found_blurb'))\n",
    "    blurb_perc.append(float(sum(sub['flag']=='Found_blurb'))/float(len(sub)))\n",
    "    blurb_perc_of_urls.append(float(sum(sub['flag']=='Found_blurb'))/float(sum(sub['scraped_dataset']==True)))\n",
    "    sub_blurb = sub[sub['flag']=='Found_blurb']\n",
    "    med_tot.append(len(sub_blurb[(sub_blurb['length']>=250) & (sub_blurb['length']<=4000)]))\n",
    "    med_perc.append(float(len(sub_blurb[(sub_blurb['length']>=250) & (sub_blurb['length']<=4000)]))/float(len(sub)))\n",
    "df_sch_type = pd.DataFrame({'sch_type': sch_type})\n",
    "df_sch_type['edubase_tot'] = edubase_tot\n",
    "df_sch_type['url_tot'] = url_tot\n",
    "df_sch_type['scrape_tot'] = scrape_tot\n",
    "df_sch_type['url_perc'] = url_perc\n",
    "df_sch_type['blurb_tot'] = blurb_tot\n",
    "df_sch_type['blurb_perc'] = blurb_perc\n",
    "df_sch_type['blurb_perc_of_urls'] = blurb_perc_of_urls\n",
    "df_sch_type['med_tot'] = med_tot\n",
    "df_sch_type['med_perc'] = med_perc\n",
    "df_sch_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of this table, I will drop Independent schools drop pupil referall units, studio schools, and universal technical colleges because there are so few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24223\n",
      "23527\n"
     ]
    }
   ],
   "source": [
    "print len(df)\n",
    "df = df[(df['school_type']!='LA Nursery School')&\\\n",
    "        (df['school_type']!='Pupil Referral Unit')&\\\n",
    "        (df['school_type']!='Studio Schools')]\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look scraping sample statistics for my final reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schools in EduBase dataset: 23527\n",
      "Obs for which have URLs: 20451\n",
      "Obs after scraping: 20451\n",
      "\n",
      "Looking at obs after scraping....\n",
      "URL worked but no blurb: 4078\n",
      "\n",
      "Total errors: 791\n",
      "Other error: 167\n",
      "HTTP error: 235\n",
      "DNS error: 381\n",
      "Timeout error: 8\n",
      "\n",
      "Total found blurb: 15582\n",
      "Blurbs >4k characters: 29\n",
      "Blurbs <250 characters: 10868\n",
      "Blurbs >=250 & <=4k characters: 4685\n"
     ]
    }
   ],
   "source": [
    "## Drop same collapsed school types from ed_schools\n",
    "ed_schools = collapse_types(ed_schools)\n",
    "ed_schools = df[(df['school_type']!='LA Nursery School')&\\\n",
    "                (df['school_type']!='Pupil Referral Unit')&\\\n",
    "                (df['school_type']!='Studio Schools')]\n",
    "\n",
    "## Get list of additional school types to drop from ed_schools\n",
    "sch_drops = []\n",
    "for school_type in sorted(list(ed_other['TypeOfEstablishment'].unique())):\n",
    "    sch_drops.append(school_type)\n",
    "sch_drops = [x for x in sch_drops if x != 'Other Independent School']\n",
    "sch_drops = [x for x in sch_drops if x != 'Other Independent Special School']\n",
    "\n",
    "## Drop those school types and check has same number of school types as df\n",
    "for school_type in sch_drops:\n",
    "    ed_schools = ed_schools[(ed_schools['TypeOfEstablishment']!=school_type)]\n",
    "len(sorted(list(ed_schools['TypeOfEstablishment'].unique()))) == \\\n",
    "len(sorted(list(df['TypeOfEstablishment'].unique())))\n",
    "\n",
    "## Get scraped sample summary stats\n",
    "scrapy_stats(ed_schools, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 23527\n",
      "Rows with NaN in blurb: 7945\n",
      "Non-null rows: 15582\n",
      "Non-short rows: 5140\n",
      "        URN                            SchoolWebsite  \\\n",
      "5    100009       http://www.beckford.camden.sch.uk/   \n",
      "22   100029      http://www.cchurchnw1.camden.sch.uk   \n",
      "31   100039     http://www.stdominics.camden.sch.uk/   \n",
      "32   100040       http://www.stgeorge.camden.sch.uk/   \n",
      "38   100046        http://www.stpauls.camden.sch.uk/   \n",
      "43   100051      http://www.regenthighschool.org.uk/   \n",
      "49   100059         http://www.lasainteunion.org.uk/   \n",
      "52   100092                http://ccfl.camden.sch.uk   \n",
      "53   100094      http://www.royalfree.camden.sch.uk/   \n",
      "54   100096           www.swisscottage.camden.sch.uk   \n",
      "63   100115           www.cherryorchardschool.org.uk   \n",
      "68   100128                   www.greenacres.org.uk/   \n",
      "69   100129      http://www.haimoprimaryschool.co.uk   \n",
      "77   100141             www.sheringtonprimary.co.uk/   \n",
      "78   100142          www.thorntree.greenwich.sch.uk/   \n",
      "83   100151                www.rockliffemanor.co.uk/   \n",
      "90   100160    http://www.nightingaleprimary.org.uk/   \n",
      "96   100167        http://www.elthamcoeschool.co.uk/   \n",
      "98   100169          www.stjosephs.greenwich.sch.uk/   \n",
      "105  100176                      www.stmprimary.org/   \n",
      "106  100177           www.stthomasabecketsch.org.uk/   \n",
      "110  100181     www.bishopjohnrobinsonprimary.co.uk/   \n",
      "111  100182  http://www.elthamhill.greenwich.sch.uk/   \n",
      "120  100218                www.berger.hackney.sch.uk   \n",
      "124  100225         www.gainsborough.hackney.sch.uk/   \n",
      "125  100229             www.lauriston.hackney.sch.uk   \n",
      "129  100234              www.orchard.hackney.sch.uk/   \n",
      "132  100239          www.princessmay.hackney.sch.uk/   \n",
      "135  100242            www.southwold.hackney.sch.uk/   \n",
      "140  100250     http://www.kingsmead.hackney.sch.uk/   \n",
      "141  100251       www.sirthomasabney.hackney.sch.uk/   \n",
      "145  100255         www.baden-powell.hackney.sch.uk/   \n",
      "150  100260              www.benthal.hackney.sch.uk/   \n",
      "151  100261            www.mandeville.hackney.sch.uk   \n",
      "152  100263          www.holytrinity.hackney.sch.uk/   \n",
      "160  100274       www.st-scholasticas.hackney.sch.uk   \n",
      "177  100326         www.milescoverdaleprimary.co.uk/   \n",
      "178  100328             www.floragardens.lbhf.sch.uk   \n",
      "182  100338          www.sirjohnlillieprimary.co.uk/   \n",
      "186  100345       http://www.holycrossfulham.org.uk/   \n",
      "189  100349                www.stjohnsce.lbhf.sch.uk   \n",
      "190  100350                www.stmarysrc.lbhf.sch.uk   \n",
      "192  100352              www.stpetersce.lbhf.sch.uk/   \n",
      "196  100357              www.stthomasrc.lbhf.sch.uk/   \n",
      "199  100381               www.jacktizard.lbhf.sch.uk   \n",
      "213  100408                    www.hargravepark.com/   \n",
      "214  100411            www.laycock.islington.sch.uk/   \n",
      "217  100422         http://www.robertblairschool.com   \n",
      "218  100425           www.thornhill.islington.sch.uk   \n",
      "220  100428              www.wintonprimaryschool.com   \n",
      "\n",
      "          TypeOfEstablishment EstablishmentStatus  length  \\\n",
      "5            Community School                Open   225.0   \n",
      "22     Voluntary Aided School                Open   415.0   \n",
      "31     Voluntary Aided School                Open   229.0   \n",
      "32     Voluntary Aided School                Open   223.0   \n",
      "38     Voluntary Aided School                Open   911.0   \n",
      "43           Community School                Open   812.0   \n",
      "49     Voluntary Aided School                Open   317.0   \n",
      "52   Community Special School                Open  1000.0   \n",
      "53   Community Special School                Open   821.0   \n",
      "54   Community Special School                Open   963.0   \n",
      "63           Community School                Open   428.0   \n",
      "68           Community School                Open  1498.0   \n",
      "69           Community School                Open  1751.0   \n",
      "77           Community School                Open   311.0   \n",
      "78           Community School                Open   616.0   \n",
      "83           Community School                Open   446.0   \n",
      "90           Community School                Open   838.0   \n",
      "96     Voluntary Aided School                Open   374.0   \n",
      "98     Voluntary Aided School                Open   266.0   \n",
      "105    Voluntary Aided School                Open   396.0   \n",
      "106    Voluntary Aided School                Open  1267.0   \n",
      "110    Voluntary Aided School                Open   220.0   \n",
      "111          Community School                Open   453.0   \n",
      "120          Community School                Open   226.0   \n",
      "124          Community School                Open   216.0   \n",
      "125          Community School                Open   486.0   \n",
      "129          Community School                Open   646.0   \n",
      "132          Community School                Open   567.0   \n",
      "135          Community School                Open   648.0   \n",
      "140          Community School                Open   935.0   \n",
      "141          Community School                Open   984.0   \n",
      "145          Community School                Open  1015.0   \n",
      "150          Community School                Open   927.0   \n",
      "151          Community School                Open   257.0   \n",
      "152    Voluntary Aided School                Open   400.0   \n",
      "160    Voluntary Aided School                Open   273.0   \n",
      "177          Community School                Open   568.0   \n",
      "178          Community School                Open   546.0   \n",
      "182          Community School                Open   504.0   \n",
      "186    Voluntary Aided School                Open   438.0   \n",
      "189    Voluntary Aided School                Open   755.0   \n",
      "190    Voluntary Aided School                Open   818.0   \n",
      "192    Voluntary Aided School                Open  1239.0   \n",
      "196    Voluntary Aided School                Open   719.0   \n",
      "199  Community Special School                Open  1332.0   \n",
      "213          Community School                Open  1134.0   \n",
      "214          Community School                Open   220.0   \n",
      "217          Community School                Open   890.0   \n",
      "218          Community School                Open   399.0   \n",
      "220          Community School                Open   960.0   \n",
      "\n",
      "                           time  \\\n",
      "5    2017-02-18 16:33:33.446440   \n",
      "22   2017-02-18 16:33:34.014199   \n",
      "31   2017-02-18 16:33:34.876158   \n",
      "32   2017-02-18 16:33:34.157462   \n",
      "38   2017-02-18 16:33:35.150168   \n",
      "43   2017-02-18 16:33:35.091818   \n",
      "49   2017-02-18 16:33:35.962751   \n",
      "52   2017-02-18 16:33:36.729026   \n",
      "53   2017-02-18 16:33:36.370937   \n",
      "54   2017-02-18 16:33:46.652477   \n",
      "63   2017-02-18 16:33:36.361874   \n",
      "68   2017-02-18 16:33:38.342416   \n",
      "69   2017-02-18 16:33:37.035926   \n",
      "77   2017-02-18 16:33:47.349320   \n",
      "78   2017-02-18 16:33:38.367366   \n",
      "83   2017-02-18 16:33:37.123858   \n",
      "90   2017-02-18 16:33:38.425517   \n",
      "96   2017-02-18 16:33:46.966607   \n",
      "98   2017-02-18 16:33:46.900074   \n",
      "105  2017-02-18 16:33:38.531696   \n",
      "106  2017-02-18 16:33:38.596056   \n",
      "110  2017-02-18 16:33:41.848626   \n",
      "111  2017-02-18 16:34:08.271490   \n",
      "120  2017-02-18 16:33:46.467737   \n",
      "124  2017-02-18 16:33:42.306741   \n",
      "125  2017-02-18 16:33:38.941267   \n",
      "129  2017-02-18 16:33:42.903016   \n",
      "132  2017-02-18 16:33:42.807545   \n",
      "135  2017-02-18 16:33:42.335101   \n",
      "140  2017-02-18 16:33:45.450840   \n",
      "141  2017-02-18 16:33:48.649835   \n",
      "145  2017-02-18 16:33:43.683882   \n",
      "150  2017-02-18 16:33:47.928650   \n",
      "151  2017-02-18 16:33:46.974275   \n",
      "152  2017-02-18 16:33:45.656125   \n",
      "160  2017-02-18 16:33:55.887455   \n",
      "177  2017-02-18 16:33:57.739971   \n",
      "178  2017-02-18 16:33:47.646307   \n",
      "182  2017-02-18 16:33:45.715929   \n",
      "186  2017-02-18 16:33:46.930271   \n",
      "189  2017-02-18 16:33:46.245042   \n",
      "190  2017-02-18 16:33:47.641112   \n",
      "192  2017-02-18 16:33:47.373113   \n",
      "196  2017-02-18 16:33:48.386832   \n",
      "199  2017-02-18 16:33:50.259613   \n",
      "213  2017-02-18 16:33:53.884960   \n",
      "214  2017-02-18 16:34:57.875115   \n",
      "217  2017-02-18 16:33:54.777664   \n",
      "218  2017-02-18 16:33:53.079260   \n",
      "220  2017-02-18 16:33:54.656814   \n",
      "\n",
      "                                                   url  \\\n",
      "5    http://www.beckford.camden.sch.uk/page/default...   \n",
      "22                 http://www.cchurchnw1.camden.sch.uk   \n",
      "31                http://www.stdominics.camden.sch.uk/   \n",
      "32                  http://www.stgeorge.camden.sch.uk/   \n",
      "38                   http://www.stpauls.camden.sch.uk/   \n",
      "43                 http://www.regenthighschool.org.uk/   \n",
      "49                    http://www.lasainteunion.org.uk/   \n",
      "52                           http://ccfl.camden.sch.uk   \n",
      "53   http://www.royalfree.camden.sch.uk/page/defaul...   \n",
      "54                  http://swisscottage.camden.sch.uk/   \n",
      "63               http://www.cherryorchardschool.org.uk   \n",
      "68                       http://www.greenacres.org.uk/   \n",
      "69                 http://www.haimoprimaryschool.co.uk   \n",
      "77                     http://sheringtonprimary.co.uk/   \n",
      "78              http://www.thorntree.greenwich.sch.uk/   \n",
      "83                    http://www.rockliffemanor.co.uk/   \n",
      "90               http://www.nightingaleprimary.org.uk/   \n",
      "96                       http://elthamcoeschool.co.uk/   \n",
      "98                  http://stjosephs.greenwich.sch.uk/   \n",
      "105                         http://www.stmprimary.org/   \n",
      "106              http://www.stthomasabecketsch.org.uk/   \n",
      "110  http://www.bishopjohnrobinsonprimary.co.uk/pag...   \n",
      "111                             http://elthamhill.com/   \n",
      "120                   http://www.berger.hackney.sch.uk   \n",
      "124            http://www.gainsborough.hackney.sch.uk/   \n",
      "125                http://www.lauriston.hackney.sch.uk   \n",
      "129                 http://www.orchard.hackney.sch.uk/   \n",
      "132             http://www.princessmay.hackney.sch.uk/   \n",
      "135               http://www.southwold.hackney.sch.uk/   \n",
      "140  http://www.kingsmead.hackney.sch.uk/page/defau...   \n",
      "141              http://sirthomasabney.hackney.sch.uk/   \n",
      "145            http://www.baden-powell.hackney.sch.uk/   \n",
      "150                     http://benthal.hackney.sch.uk/   \n",
      "151               http://www.mandeville.hackney.sch.uk   \n",
      "152             http://www.holytrinity.hackney.sch.uk/   \n",
      "160                        http://st-scholasticas.com/   \n",
      "177            http://www.milescoverdaleprimary.co.uk/   \n",
      "178                http://www.floragardens.lbhf.sch.uk   \n",
      "182             http://www.sirjohnlillieprimary.co.uk/   \n",
      "186                 http://www.holycrossfulham.org.uk/   \n",
      "189                   http://www.stjohnsce.lbhf.sch.uk   \n",
      "190                   http://www.stmarysrc.lbhf.sch.uk   \n",
      "192                 http://www.stpetersce.lbhf.sch.uk/   \n",
      "196                 http://www.stthomasrc.lbhf.sch.uk/   \n",
      "199                  http://www.jacktizard.lbhf.sch.uk   \n",
      "213                       http://www.hargravepark.com/   \n",
      "214                   http://laycock.islington.sch.uk/   \n",
      "217                   http://www.robertblairschool.com   \n",
      "218              http://www.thornhill.islington.sch.uk   \n",
      "220                 http://www.wintonprimaryschool.com   \n",
      "\n",
      "                                        orig_url       urn         flag  \\\n",
      "5             http://www.beckford.camden.sch.uk/  100009.0  Found_blurb   \n",
      "22           http://www.cchurchnw1.camden.sch.uk  100029.0  Found_blurb   \n",
      "31          http://www.stdominics.camden.sch.uk/  100039.0  Found_blurb   \n",
      "32            http://www.stgeorge.camden.sch.uk/  100040.0  Found_blurb   \n",
      "38             http://www.stpauls.camden.sch.uk/  100046.0  Found_blurb   \n",
      "43           http://www.regenthighschool.org.uk/  100051.0  Found_blurb   \n",
      "49              http://www.lasainteunion.org.uk/  100059.0  Found_blurb   \n",
      "52                     http://ccfl.camden.sch.uk  100092.0  Found_blurb   \n",
      "53           http://www.royalfree.camden.sch.uk/  100094.0  Found_blurb   \n",
      "54         http://www.swisscottage.camden.sch.uk  100096.0  Found_blurb   \n",
      "63         http://www.cherryorchardschool.org.uk  100115.0  Found_blurb   \n",
      "68                 http://www.greenacres.org.uk/  100128.0  Found_blurb   \n",
      "69           http://www.haimoprimaryschool.co.uk  100129.0  Found_blurb   \n",
      "77           http://www.sheringtonprimary.co.uk/  100141.0  Found_blurb   \n",
      "78        http://www.thorntree.greenwich.sch.uk/  100142.0  Found_blurb   \n",
      "83              http://www.rockliffemanor.co.uk/  100151.0  Found_blurb   \n",
      "90         http://www.nightingaleprimary.org.uk/  100160.0  Found_blurb   \n",
      "96             http://www.elthamcoeschool.co.uk/  100167.0  Found_blurb   \n",
      "98        http://www.stjosephs.greenwich.sch.uk/  100169.0  Found_blurb   \n",
      "105                   http://www.stmprimary.org/  100176.0  Found_blurb   \n",
      "106        http://www.stthomasabecketsch.org.uk/  100177.0  Found_blurb   \n",
      "110  http://www.bishopjohnrobinsonprimary.co.uk/  100181.0  Found_blurb   \n",
      "111      http://www.elthamhill.greenwich.sch.uk/  100182.0  Found_blurb   \n",
      "120             http://www.berger.hackney.sch.uk  100218.0  Found_blurb   \n",
      "124      http://www.gainsborough.hackney.sch.uk/  100225.0  Found_blurb   \n",
      "125          http://www.lauriston.hackney.sch.uk  100229.0  Found_blurb   \n",
      "129           http://www.orchard.hackney.sch.uk/  100234.0  Found_blurb   \n",
      "132       http://www.princessmay.hackney.sch.uk/  100239.0  Found_blurb   \n",
      "135         http://www.southwold.hackney.sch.uk/  100242.0  Found_blurb   \n",
      "140         http://www.kingsmead.hackney.sch.uk/  100250.0  Found_blurb   \n",
      "141    http://www.sirthomasabney.hackney.sch.uk/  100251.0  Found_blurb   \n",
      "145      http://www.baden-powell.hackney.sch.uk/  100255.0  Found_blurb   \n",
      "150           http://www.benthal.hackney.sch.uk/  100260.0  Found_blurb   \n",
      "151         http://www.mandeville.hackney.sch.uk  100261.0  Found_blurb   \n",
      "152       http://www.holytrinity.hackney.sch.uk/  100263.0  Found_blurb   \n",
      "160    http://www.st-scholasticas.hackney.sch.uk  100274.0  Found_blurb   \n",
      "177      http://www.milescoverdaleprimary.co.uk/  100326.0  Found_blurb   \n",
      "178          http://www.floragardens.lbhf.sch.uk  100328.0  Found_blurb   \n",
      "182       http://www.sirjohnlillieprimary.co.uk/  100338.0  Found_blurb   \n",
      "186           http://www.holycrossfulham.org.uk/  100345.0  Found_blurb   \n",
      "189             http://www.stjohnsce.lbhf.sch.uk  100349.0  Found_blurb   \n",
      "190             http://www.stmarysrc.lbhf.sch.uk  100350.0  Found_blurb   \n",
      "192           http://www.stpetersce.lbhf.sch.uk/  100352.0  Found_blurb   \n",
      "196           http://www.stthomasrc.lbhf.sch.uk/  100357.0  Found_blurb   \n",
      "199            http://www.jacktizard.lbhf.sch.uk  100381.0  Found_blurb   \n",
      "213                 http://www.hargravepark.com/  100408.0  Found_blurb   \n",
      "214         http://www.laycock.islington.sch.uk/  100411.0  Found_blurb   \n",
      "217             http://www.robertblairschool.com  100422.0  Found_blurb   \n",
      "218        http://www.thornhill.islington.sch.uk  100425.0  Found_blurb   \n",
      "220           http://www.wintonprimaryschool.com  100428.0  Found_blurb   \n",
      "\n",
      "                                                 blurb scraped_dataset  \\\n",
      "5    Welcome to the Beckford Primary School website...            True   \n",
      "22   Welcome to Christ Church Primary School. Pleas...            True   \n",
      "31   Welcome to St Dominic's Catholic Primary Schoo...            True   \n",
      "32   Welcome to our school. We are a one form entry...            True   \n",
      "38   Welcome to St Paul's CE Primary School Headtea...            True   \n",
      "43   Welcome from the Headteacher I am delighted to...            True   \n",
      "49   Welcome to La Sainte Union. We are a Catholic ...            True   \n",
      "52   Welcome to Camden Centre for Learning website....            True   \n",
      "53   Welcome The Royal Free Hospital Children's Sch...            True   \n",
      "54   Welcome to Swiss Cottage School, Development a...            True   \n",
      "63   Welcome to Cherry Orchard Primary School's web...            True   \n",
      "68   Welcome to Greenacres Primary School Welcome t...            True   \n",
      "69   Welcome On behalf of the pupils, staff and gov...            True   \n",
      "77   Welcome to Sheringtons website, Sherington is...            True   \n",
      "78   Welcome to our website At Thorntree we pride o...            True   \n",
      "83   Welcome to the Rockliffe Manor Primary School ...            True   \n",
      "90   Welcome to Nightingale Primary School. We are ...            True   \n",
      "96   Welcome to Eltham Church of England Primary Sc...            True   \n",
      "98   Welcome to the St Joseph's Primary School Webs...            True   \n",
      "105  Welcome to St Thomas More Catholic Primary Sch...            True   \n",
      "106  Welcome to St Thomas a Becket School This is a...            True   \n",
      "110  Welcome to Bishop John Robinson Church of Engl...            True   \n",
      "111  Principal's Welcome At Eltham Hill School, an ...            True   \n",
      "120  Welcome to Berger Primary School Berger Primar...            True   \n",
      "124  Welcome to our new website. Were very excited...            True   \n",
      "125  Welcome To Lauriston School. I am the Executiv...            True   \n",
      "129  Welcome Welcome to Orchard. We hope that you f...            True   \n",
      "132  Welcome to Primary School At Princess May, we ...            True   \n",
      "135  Welcome Welcome to Southwold. We hope that you...            True   \n",
      "140  Welcome Welcome to the website of Kingsmead Pr...            True   \n",
      "141  Welcome to where every child is cared for, val...            True   \n",
      "145  - Headteachers Welcome - Mission and Vision S...            True   \n",
      "150  - Headteachers Welcome - School Aims and Visi...            True   \n",
      "151  Welcome to our new website. Here you will find...            True   \n",
      "152  Welcome to Holy Trinity Holy Trinity is a one ...            True   \n",
      "160  Welcome to St Scholastica's Primary School Wel...            True   \n",
      "177  Welcome to the Miles Coverdale Primary School ...            True   \n",
      "178  Welcome! Flora Gardens is an inclusive, one-fo...            True   \n",
      "182  Welcome to the Sir John Lillie Primary School ...            True   \n",
      "186  Welcome to Holy Cross Catholic Primary Dear Pa...            True   \n",
      "189  Welcome As Headteacher it gives me great pleas...            True   \n",
      "190  Welcome to our school website. We hope it prov...            True   \n",
      "192  Welcome to St Peter's Church of England Primar...            True   \n",
      "196  Heads Welcome A very warm welcome to our webs...            True   \n",
      "199  Welcome to Jack Tizard School. We are an 'Outs...            True   \n",
      "213  Welcome to Hargrave Park Schools website! Sit...            True   \n",
      "214  Welcome to the Laycock Primary School Website ...            True   \n",
      "217  Welcome to Robert Blair Primary School and Chi...            True   \n",
      "218  Welcome Welcome to Thornhill Primary School. W...            True   \n",
      "220  Welcome to Winton Primary School I am very ple...            True   \n",
      "\n",
      "                school_type  \n",
      "5                 Community  \n",
      "22   Voluntary Aided School  \n",
      "31   Voluntary Aided School  \n",
      "32   Voluntary Aided School  \n",
      "38   Voluntary Aided School  \n",
      "43                Community  \n",
      "49   Voluntary Aided School  \n",
      "52                Community  \n",
      "53                Community  \n",
      "54                Community  \n",
      "63                Community  \n",
      "68                Community  \n",
      "69                Community  \n",
      "77                Community  \n",
      "78                Community  \n",
      "83                Community  \n",
      "90                Community  \n",
      "96   Voluntary Aided School  \n",
      "98   Voluntary Aided School  \n",
      "105  Voluntary Aided School  \n",
      "106  Voluntary Aided School  \n",
      "110  Voluntary Aided School  \n",
      "111               Community  \n",
      "120               Community  \n",
      "124               Community  \n",
      "125               Community  \n",
      "129               Community  \n",
      "132               Community  \n",
      "135               Community  \n",
      "140               Community  \n",
      "141               Community  \n",
      "145               Community  \n",
      "150               Community  \n",
      "151               Community  \n",
      "152  Voluntary Aided School  \n",
      "160  Voluntary Aided School  \n",
      "177               Community  \n",
      "178               Community  \n",
      "182               Community  \n",
      "186  Voluntary Aided School  \n",
      "189  Voluntary Aided School  \n",
      "190  Voluntary Aided School  \n",
      "192  Voluntary Aided School  \n",
      "196  Voluntary Aided School  \n",
      "199               Community  \n",
      "213               Community  \n",
      "214               Community  \n",
      "217               Community  \n",
      "218               Community  \n",
      "220               Community  \n"
     ]
    }
   ],
   "source": [
    "# Drop null values in blurb column\n",
    "print 'Total rows:', len(df)\n",
    "print 'Rows with NaN in blurb:', len(df[df.blurb.isnull()])\n",
    "df = df[df.blurb.notnull()]\n",
    "print 'Non-null rows:', len(df)\n",
    "\n",
    "# Drop rows where blurb shorter than 200 characters (or not there at all)\n",
    "df = df[df.blurb.str.len() > 199]\n",
    "print 'Non-short rows:', len(df)\n",
    "\n",
    "##Get list of URNs (will need later when put it back together) and blurbs\n",
    "urn_list = df[\"urn\"].tolist()\n",
    "blist = df[\"blurb\"].tolist()\n",
    "print df.head(50)\n",
    "\n",
    "##Remove all non-ASCII characters from blurbs (eg. \\xe2\\x80\\x98)\n",
    "new_list = []\n",
    "for b in blist:\n",
    "    new_list.append(b.decode('utf8').encode('ascii', errors='ignore'))\n",
    "blist_orig = new_list\n",
    "\n",
    "##For now, let's work with the first 200 of that list\n",
    "blist = blist_orig[:200]\n",
    "urn_list = urn_list[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../my_datasets/prepped.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions and stemmer to preprocess data\n",
    "\n",
    "In both my k-means and LDA analysis below, for each blurb in my list, I need to (in the following order):\n",
    "<ul>\n",
    "<li>**Tokenize**: divide string into a list of substrings </li>\n",
    "<li>**Remove stopwords**: stopwords are a list of high frequency words like, the, to, and also</li>\n",
    "<li>**Stem**: take the root of each word</li>\n",
    "</ul>\n",
    "\n",
    "In addition, I make all letters lower case and drop punctuation, and make sure each string/token contains letters and is longer than two characters. I therefore create the following functions, which I read in when doing by analysis for both k-means and LDA.\n",
    "\n",
    "```Python\n",
    "## .translate only takes str, whereas TfidfVectorizer only takes unicode.\n",
    "## Therefore, to use .translate in the tokenizer in TfidfVectorizer I need to\n",
    "## write a function that converts unicode -> string, applies .translate,\n",
    "## and then converts it back\n",
    "def no_punctuation_unicode(text):\n",
    "    str_text = str(text)\n",
    "    no_punctuation = str_text.translate(None, string.punctuation)\n",
    "    unicode_text = no_punctuation.decode('utf-8')\n",
    "    return unicode_text\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def prep_blurb(text):\n",
    "    lowers = text.lower()\n",
    "    no_punct = no_punctuation_unicode(lowers)\n",
    "    tokens = nltk.word_tokenize(no_punct)\n",
    "    has_letters = [t for t in tokens if re.search('[a-zA-Z]',t)]\n",
    "    no_numbers  = [t for t in has_letters if not hasNumbers(t)]\n",
    "    drop_stops = [w for w in no_numbers if not w in stoplist] \n",
    "    stems = [stemmer.stem(t) for t in drop_stops]\n",
    "    drop_short = [s for s in stems if len(s)>2]  ##not sure about this line\n",
    "    return drop_short\n",
    "\n",
    "## Get stemmer outside of prep_blurb() function, so don't have to re-do every time I use the function\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "I should like to obtain the URLs for more schools. I could do this my searching for schools and taking the top results using the Bing API.\n",
    "\n",
    "I should also like to try to improve my scraper to capture the blurbs from a higher proportion of school websites. \n",
    "\n",
    "To do improve my scraper, a first step might be to look through some of the websites for which I failed to scrape successfull. The code in the cell below is old (and thus probalby won't run if commented out). But it shows how to build a scraper to open a list of urls in Chrome, which would speed up looking through examples of websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import webbrowser\n",
    "# import pandas as pd\n",
    "# import random\n",
    "\n",
    "# # Read in cleaned and prepared urls\n",
    "# url_dat = pd.read_csv('../Scraping/csvurls.csv', names=['URN','URL'])\n",
    "\n",
    "# # Randomly select 100 urls (s)\n",
    "# index_vals = range(len(url_dat))\n",
    "# random.seed(a=1.0) # et seed so can replicated analysis\n",
    "# random.shuffle(index_vals)\n",
    "# url_dat = url_dat.ix[index_vals[:4]]\n",
    "# url_dat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Open a url in turn, categorize it, then move onto the next url\n",
    "# # (unfortunately, can't get Python to close a Chrome tab, so I have to do \n",
    "# # this manually as I go)\n",
    "# blurb_type_list = []\n",
    "# for i in range(range(len(url_dat))):\n",
    "#     url = url_dat.loc[i,'URL']\n",
    "#     print url\n",
    "#     chrome_path = 'open -a /Applications/Google\\ Chrome.app %s'\n",
    "#     webbrowser.get(chrome_path).open(url)\n",
    "#     blurb_type = raw_input('Blurb type (1=welcome, 2=head, 3=welcome+head, 4=other_blurb, 5=no_blurb, 6=broken_url: ')\n",
    "#     blurb_type_list.append(blurb_type)\n",
    "    \n",
    "# # Attach blurb categorizations to data frame\n",
    "# url_dat['blurb_type'] = pd.Series( blurb_type_list, index=url_dat.index)\n",
    "\n",
    "# # Write to csv\n",
    "# url_dat.to_csv('../blurb_cats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "\n",
    "## Brief overview of k-means\n",
    "\n",
    "The k-means algorithm first chooses an initial set of centroids.\n",
    "\n",
    "After initialization, the k-means algorithm iterates between the following two steps until convergence:\n",
    "1. Assign each data point to the closest centroid.\n",
    "$$\n",
    "z_i \\gets \\mathrm{argmin}_j \\|\\mu_j - \\mathbf{x}_i\\|^2\n",
    "$$\n",
    "2. Revise centroids as the mean of the assigned data points.\n",
    "$$\n",
    "\\mu_j \\gets \\frac{1}{n_j}\\sum_{i:z_i=j} \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "Important decision that we have to make in setting up the algorithm include:\n",
    "<ul>\n",
    "<li> the number of clusters, k\n",
    "<li> the initial set of centroids - k-means converges to a local optimum, and is therefore sensitive to initialization.\n",
    "</ul>\n",
    "\n",
    "## Preprocess data\n",
    "\n",
    "### Create TF-IDF matrix\n",
    "\n",
    "To analyse this data, I need to convert it into numerical features. I next, therefore, extract a matrix of TF-IDF (term frequency-inverse document frequency) features.\n",
    "\n",
    "The tf-idf weight of a term in a document is the product of its tf (term frequency) weight and its idf (inverse document frequency) weight. This is generally given by the formula:\n",
    "\n",
    "$w_{t,d} = (1 + logtf_{t,d}) \\times log_{10}(\\frac{N}{df_t})$\n",
    "\n",
    "Where the term frequency $tf_{t,d}$ of term t in document d is defined as the number of times that t occurs in d, the document frequency $df_t$ is he number of documents that contain t, and N is the number of documents. The computation of tf-idfs in scikit-learn is in fact [slightly different from the standard textbook notation](http://scikit-learn.org/dev/modules/feature_extraction.html#the-bag-of-words-representation), but we can ignore that here.\n",
    "\n",
    "The tf-idf weight (of a term in a document) increases with the number of times a term occurs in a document, and also increases with the rarity of the term in the collection. Jurafsky and Manning have a [few videos](https://www.youtube.com/watch?v=PhunzHqhKoQ) giving a nice introduction to to tf-idf weighting and its components.\n",
    "\n",
    "Extracting tf-idf features gives us a weight matrix between terms and documents. Each document is represented by a row of the matrix, which is a real valued vector, and each term by a column. This will be a sparse matrix, that is, a matrix in which most of the elements are zero\n",
    "\n",
    "scikit-learn's TfidfVectorizer returns a matrix in scipy.sparse.csr format. Printing this matrix shows the nonzero values of the matrix in the (row, col) format.\n",
    "\n",
    "A few things to note about the parameters I define below (or previously experimented with defining):\n",
    "<ul>\n",
    "<li> max_df: the maximum frequency within the documents a given term can have to be used in the tfi-idf matrix.I stick with the default of 1.\n",
    "<li> min_idf: is the minimum frequecy a term can have to be included in the matrix. Can pass it either an integer (eg must occur 7 times) or a decimal (eg. must occur in at least .2 of the documents). I stick with the default of 1 (ie only needs to occur once).\n",
    "<li> ngram_range: the lower and upper boundary of the range of n-values for different n-grams to be extracted. An n-gram is basically a set of co-occuring words, and you typically move one for more word forward. For example, if n=2 (known as bigrams) then for the sentence \"The cow jumps over the moon\" the bigrams would be \"the cose\" \"cow jumps\" \"jumps over\" \"over the\" \"the moon\". I stick wiht the default of (1,1), that is, I only look at individual words.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 14.7 ms, total: 1.28 s\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "##Need to convert items in blist, as TfidfVectorizer only takes unicode\n",
    "blist = [b.decode('utf-8') for b in blist]\n",
    "\n",
    "##Create tf-idf matrix\n",
    "\n",
    "##Stopwords must be in unicode to work with TfidfVectorizer)\n",
    "stoplist = [word.decode('utf-8') for word in nltk.corpus.stopwords.words('english')] \n",
    "\n",
    "##Don't use stop_words argument to TfidfVectorizer as delt with in prep_blurb\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=prep_blurb)\n",
    "\n",
    "%time tf_idf = tfidf_vectorizer.fit_transform(blist)\n",
    "\n",
    "list_of_words = tfidf_vectorizer.get_feature_names()\n",
    "# print(tf_idf.shape)\n",
    "# print type(tf_idf)\n",
    "# print tfidf_matrix[:10,]\n",
    "# print list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize all vectors\n",
    "\n",
    "Euclidean distance can be a poor metric of similarity between text documents, as it unfairly penalizes long articles. For a reasonable assessment of similarity, we should disregard the length information and use length-agnostic metrics, such as cosine distance. The k-means algorithm does not directly work with cosine distance, so I take an alternative route to remove length information: I normalize all vectors to be unit length. Euclidean distance closely mimics cosine distance when all vectors are unit length. In particular, the squared Euclidean distance between any two vectors of length one is directly proportional to their cosine distance.\n",
    "\n",
    "I use the normalize() function from scikit-learn to normalize all vectors to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fit initial k-means model\n",
    "\n",
    "I will start by using k-means++ for initialization and specifying 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 254 ms, sys: 13.3 ms, total: 267 ms\n",
      "Wall time: 223 ms\n",
      "\n",
      "Coordinates of cluster centers: [[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.00378007  0.         ...,  0.          0.00253571  0.        ]\n",
      " [ 0.          0.          0.01231351 ...,  0.0138358   0.          0.00614341]\n",
      " [ 0.07410287  0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.00233756  0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      "Label of each data point\n",
      "[1 1 4 1 1 4 1 2 4 2 2 2 1 4 1 1 1 1 1 1 1 4 1 1 1 4 1 1 1 1 1 1 1 1 2 1 1\n",
      " 4 4 1 1 2 1 2 1 1 4 1 1 1 1 4 1 1 1 1 1 1 1 1 4 2 4 1 1 1 1 4 1 1 1 4 2 1\n",
      " 4 1 2 1 4 4 1 1 1 2 1 1 1 1 2 2 1 4 2 1 1 1 4 1 1 4 1 1 2 2 1 1 1 2 4 1 2\n",
      " 4 1 1 4 4 3 1 4 1 3 1 2 1 2 2 2 1 1 4 1 2 4 1 1 1 1 1 4 4 1 1 1 1 4 0 1 1\n",
      " 4 1 1 1 1 1 1 1 1 4 0 4 2 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 4 1 3 1 1 1 1 4 1\n",
      " 3 3 3 1 4 1 2 1 3 1 2 2 1 1 2]\n",
      "\n",
      "Label of each data point\n",
      "[1 1 4 1 1 4 1 2 4 2 2 2 1 4 1 1 1 1 1 1 1 4 1 1 1 4 1 1 1 1 1 1 1 1 2 1 1\n",
      " 4 4 1 1 2 1 2 1 1 4 1 1 1 1 4 1 1 1 1 1 1 1 1 4 2 4 1 1 1 1 4 1 1 1 4 2 1\n",
      " 4 1 2 1 4 4 1 1 1 2 1 1 1 1 2 2 1 4 2 1 1 1 4 1 1 4 1 1 2 2 1 1 1 2 4 1 2\n",
      " 4 1 1 4 4 3 1 4 1 3 1 2 1 2 2 2 1 1 4 1 2 4 1 1 1 1 1 4 4 1 1 1 1 4 0 1 1\n",
      " 4 1 1 1 1 1 1 1 1 4 0 4 2 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 4 1 3 1 1 1 1 4 1\n",
      " 3 3 3 1 4 1 2 1 3 1 2 2 1 1 2]\n",
      "\n",
      "Sum of distances of samples to their closest cluster center\n",
      "174.641566226\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=5, init='k-means++')\n",
    "%time km = kmeans.fit(tf_idf)\n",
    "print\n",
    "\n",
    "print 'Coordinates of cluster centers:', \n",
    "print km.cluster_centers_\n",
    "print \n",
    "\n",
    "print 'Label of each data point' \n",
    "print km.labels_\n",
    "print \n",
    "\n",
    "print 'Label of each data point' \n",
    "print kmeans.predict(tf_idf) \n",
    "print\n",
    "\n",
    "print 'Sum of distances of samples to their closest cluster center'\n",
    "print kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose k (number of clusters)\n",
    "\n",
    "An important choice we have to make is the number of clusters (the value of k). \n",
    "\n",
    "To compare different valus of k, I need some measure of the performance of different clusterings. One measure I can use is the sum of all squared distances between data points and centroids:\n",
    "$$\n",
    "J(\\mathcal{Z},\\mu) = \\sum_{j=1}^k \\sum_{i:z_i = j} \\|\\mathbf{x}_i - \\mu_j\\|^2.\n",
    "$$\n",
    "\n",
    "This measure makes a lot of sense, since the sum of squared distances between our observations and the cluster centers is the thing that k-means is trying to minimize. I will refer to this measure as cluster heterogenity (the sum of the heterogeneities over all clusters). The larger the distances, the more hetergoenous the clusters are, and the smaller the distances the more homogenous. We would like homogenous/tight clusters. A word of caution with this terminology: the scikit-learn [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score) also uses homogeneity to refer to a particular metric of cluster labeling given the truth.\n",
    "\n",
    "A higher value of k reduces the possible heterogeneity metric by definition, all else equal.  For example, if we have N data points and set k=N clusters, then we could have 0 cluster heterogeneity by setting the N centroids equal to the values of the N data points. One heuristic you can use to choose k is to plot heterogeneity against k and look for the \"elbow\" of the curve. This naturally trades off between trying to minimize heterogeneity, but reduce model complexity.\n",
    "\n",
    "In practice, not all runs for larger k will result in lower heterogenity than a single run with smaller k due to local optima (and thus the importance of initialization). In plotting heterogeneity against k, therefore, for each value of k I will compute heterogeneity for several runs and take the lowest heterogeneity value.\n",
    "\n",
    "Out of curiosity, I also want to take a look at how long it takes to run functions for each \n",
    "value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_heterogeneities(data, kvals, reps):\n",
    "    \"\"\"Returns heterogeneities for a series of k (number of cluster) values\n",
    "    data is a tf-idf matrix\n",
    "    kvals: the k values to use\n",
    "    reps: the number of runs to do for each k value, taking the lowest each time\n",
    "    \"\"\"\n",
    "    het_list = []\n",
    "    comp_times = []\n",
    "    for k in kvals:\n",
    "        start_time = time.time()\n",
    "        sub_het_list = []\n",
    "        for r in range(reps):             \n",
    "            sub_het_list.append(KMeans(n_clusters=k, init='k-means++').fit(tf_idf).inertia_)\n",
    "        het_list.append(min(sub_het_list))\n",
    "        comp_times.append(time.time() - start_time)\n",
    "        if k % 5 == 0:\n",
    "            print k      # to check running ok if takes a while\n",
    "    return kvals, het_list, comp_times\n",
    "\n",
    "f_kvals, f_het_list, f_times = compute_heterogeneities(tf_idf, range(2,100+1), 1)\n",
    "kmeans_het_list = [f\n",
    "with open('kmeans_het_list.txt','w') as myfile:\n",
    "    json.dump(kmeans_het_list, myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('kmeans_het_list.txt','r') as infile:\n",
    "    newList = json.load(infile)\n",
    "\n",
    "def plot_heterogeneity_vs_k(k_values, heterogeneity_values):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(k_values, heterogeneity_values, linewidth=4)\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title('Heterogeneity vs. K')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_runtime_vs_k(k_values, heterogeneity_values):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(k_values, heterogeneity_values, linewidth=4)\n",
    "    plt.xlabel('Runtime')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title('Runtime vs. K')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()    \n",
    "\n",
    "plot_heterogeneity_vs_k(f_kvals, f_het_list)\n",
    "plot_runtime_vs_k(f_kvals, f_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this plot, I select a value of k=XXX.\n",
    "\n",
    "## Review K-means clusters\n",
    "\n",
    "### Examine text in clusters\n",
    "\n",
    "I next want to look at some of the text in my different clusters to see if it seems reasonable. \n",
    "\n",
    "In a good clustering of documents:\n",
    "* Documents in the same cluster should be similar.\n",
    "* Documents from different clusters should be less similar.\n",
    "\n",
    "To examine the text in my clustering I will:\n",
    "* Fetch 5 nearest neighbors of each centroid from the set of documents assigned to that cluster. I will consider these documents as being representative of the cluster.\n",
    "* Print top 5 words that have highest tf-idf weights in each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0    \n",
      "croft 0.492454641646\n",
      "allen 0.492454641646\n",
      "centr 0.327884109405\n",
      "enjoyableeduc 0.246227320823\n",
      "compet 0.246227320823\n",
      "\n",
      "Welcome to Allens Croft Children's Centre At Allens Croft Children's Centre we will give every child the support and opportunity to achieve their potential and celebrate their learning and achievements through enjoyable,educational experiences which \n",
      "\n",
      "Welcome to Allens Croft Children's Centre At Allens Croft Children's Centre we will give every child the support and opportunity to achieve their potential and celebrate their learning and achievements through enjoyable,educational experiences which \n",
      "\n",
      "Welcome to Allens Croft Children's Centre At Allens Croft Children's Centre we will give every child the support and opportunity to achieve their potential and celebrate their learning and achievements through enjoyable,educational experiences which \n",
      "\n",
      "Welcome to Allens Croft Children's Centre At Allens Croft Children's Centre we will give every child the support and opportunity to achieve their potential and celebrate their learning and achievements through enjoyable,educational experiences which \n",
      "\n",
      "Welcome to Adlington St Paul's Church of England Primary School. We are a busy but friendly school, with lots going on. This website aims to give you a taster of the school, as well as providing up to date information for parents, pupils and the wide\n",
      "\n",
      "==========================================================\n",
      "Cluster 1    \n",
      "school 0.140205584799\n",
      "welcom 0.0588667073451\n",
      "websit 0.0520779476816\n",
      "abbey 0.0460788789938\n",
      "inform 0.0416553054774\n",
      "\n",
      "Welcome to the Federation of Abbey Schools website. We hope you will find it useful and informative. We believe our school is a special place where everyone feels part of a caring, learning environment. We are very proud of all we achieve. The pupils\n",
      "\n",
      "Welcome to the Federation of Abbey Schools website. We hope you will find it useful and informative. We believe our school is a special place where everyone feels part of a caring, learning environment. We are very proud of all we achieve. The pupils\n",
      "\n",
      "Welcome to the Federation of Abbey Schools website. We hope you will find it useful and informative. We believe our school is a special place where everyone feels part of a caring, learning environment. We are very proud of all we achieve. The pupils\n",
      "\n",
      "Welcome to the Federation of Abbey Schools website. We hope you will find it useful and informative. We believe our school is a special place where everyone feels part of a caring, learning environment. We are very proud of all we achieve. The pupils\n",
      "\n",
      "Login Parents Headteacher's Message Parents Headteacher's Message Welcome to Alnwick The Duke's Middle School I have great pleasure in introducing our school website and I hope that you will find the information contained in it useful and helpful. We\n",
      "\n",
      "==========================================================\n",
      "Cluster 2    \n",
      "academi 0.182374624239\n",
      "princip 0.173169849612\n",
      "student 0.106390352619\n",
      "trust 0.0533718083947\n",
      "educ 0.0442809052504\n",
      "\n",
      "Horizons Horizons Specialist Academy Trust I am delighted to welcome you to the website of Horizons Specialist Academy Trust (HSAT) and to share with you the wide range of learning opportunities available to our pupils and students. Horizons Speciali\n",
      "\n",
      "Horizons Horizons Specialist Academy Trust I am delighted to welcome you to the website of Horizons Specialist Academy Trust (HSAT) and to share with you the wide range of learning opportunities available to our pupils and students. Horizons Speciali\n",
      "\n",
      "Principal's Welcome Welcome to All Saints' Academy At All Saints' Academy there is a real desire to achieve excellence in all that we do whilst living out our Christian values of love, peace, justice, respect, reconciliation and service to others. Ou\n",
      "\n",
      "Welcome from the Principal I am extremely proud to be the Principal of the Academy and I will work alongside the students to create an environment that encourages every member of our community to live well together. It is important to us that student\n",
      "\n",
      "Welcome to Admirals Academy Sponsored by Academy Transformation Trust We pride ourselves on our varied and rich curriculum with a commitment to uncovering the hidden potential which we believe is in every child. Our vision is to develop a 21st centur\n",
      "\n",
      "==========================================================\n",
      "Cluster 3    \n",
      "school 0.142060754819\n",
      "children 0.0984083516819\n",
      "learn 0.0529632096605\n",
      "primari 0.0527610146608\n",
      "communiti 0.0498712127562\n",
      "\n",
      "Welcome to All Saints Anglican/Methodist Primary School On behalf of our children, staff and governors I would like to extend a very warm welcome to the All Saints Primary School family. We are a small village school, based in the heart of the beauti\n",
      "\n",
      "Welcome On behalf of the children, staff and governors Id like to welcome you to All Saints CE Primary School. The school is a member of The Ark Federation and works in partnership with Beer CofE Primary School. Our Federation is also part of St Chri\n",
      "\n",
      "Welcome to Ashleigh Primary School and Nurserys website. The purpose of this website is to provide an insight into the high quality provision offered within our school. We hope it will be of interest to existing members of our community and for those\n",
      "\n",
      "Archibald Primary School A Welcome to Archibald Primary School Website from the Headteacher Dear Parents and Carers, Thank you for taking the time to visit our website. We hope you find it a useful source of information and that it gives a good insig\n",
      "\n",
      "Welcome Welcome to Bardney Church of England and Methodist Primary School On behalf of the children, staff and governors I wish you a very warm welcome. At Bardney Church of England and Methodist Primary School we believe our role is to ensure that a\n",
      "\n",
      "==========================================================\n",
      "Cluster 4    \n",
      "river 0.266189723678\n",
      "chorleywood 0.266189723678\n",
      "rickmansworth 0.266189723678\n",
      "croxley 0.266189723678\n",
      "loudwat 0.266189723678\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=5, init='k-means++', random_state=0).fit(tf_idf)  # set seed for consistent results\n",
    "centroids = km.cluster_centers_   # coordinates of cluster centers\n",
    "cluster_assignment = km.labels_   # label of every data point\n",
    "n_clusters = 5\n",
    "\n",
    "for c in xrange(n_clusters):\n",
    "    # Cluster heading\n",
    "    print('Cluster {0:d}    '.format(c))\n",
    "\n",
    "    # Print top 5 words with largest TF-IDF weights in the cluster\n",
    "    idx = centroids[c].argsort()[::-1]\n",
    "    for i in xrange(5):\n",
    "        print list_of_words[idx[i]], centroids[c,idx[i]]\n",
    "    print ('')\n",
    "    \n",
    "    # Compute distances from the centroid to all data points in the cluster\n",
    "    distances = pairwise_distances(tf_idf, [centroids[c]], metric='euclidean').flatten()\n",
    "    distances[cluster_assignment!=c] = float('inf') # remove non-members from consideration\n",
    "    nearest_neighbors = distances.argsort() # argsort() returns the indices that would sort an array\n",
    "    # For 5 nearest neighbors, print the first 250 characters of text\n",
    "    for i in xrange(5):\n",
    "        print blist[nearest_neighbors[i]][:250]\n",
    "        print ('')\n",
    "    print('==========================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe of URNs, blurbs, and their clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URN</th>\n",
       "      <th>blurbs</th>\n",
       "      <th>km_assignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119460.0</td>\n",
       "      <td>Welcome to Adlington St Paul's Church of Engla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139709.0</td>\n",
       "      <td>Welcome to Abbey Woods Academy Abbey Woods is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114864.0</td>\n",
       "      <td>We aim to make school a happy and rewarding ex...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131982.0</td>\n",
       "      <td>Menu Welcome from the Principal Principals Blo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121326.0</td>\n",
       "      <td>Welcome to Alanbrooke Community Primary School...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        URN                                             blurbs  km_assignment\n",
       "0  119460.0  Welcome to Adlington St Paul's Church of Engla...              1\n",
       "1  139709.0  Welcome to Abbey Woods Academy Abbey Woods is ...              1\n",
       "2  114864.0  We aim to make school a happy and rewarding ex...              3\n",
       "3  131982.0  Menu Welcome from the Principal Principals Blo...              2\n",
       "4  121326.0  Welcome to Alanbrooke Community Primary School...              3"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifed_blurbs = pd.DataFrame(\n",
    "    {'URN': urn_list,\n",
    "     'blurbs': blist,\n",
    "     'km_assignment': cluster_assignment\n",
    "    })\n",
    "classifed_blurbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize clusters\n",
    "\n",
    "#### Dimensionality reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent dirichlet allocation (LDA)\n",
    "\n",
    "<p>We have so far used the k-means alogrithm to assign the text blurb from each school website to a single cluster (a mixture model). But we may instead want to think of each blurb as falling into multiple clusters; for example, a blurb may talk about both academic performance and a religious culture. Mixed memership models allow use to associate a given data point with a set of different cluster assignments and to capture the relative proportion of different clusters in a datapoint. Latent dirichlet allocation (LDA) one such mixed membership model.</p>\n",
    "\n",
    "## Brief overview of LDA\n",
    "\n",
    "Here, I briefly discuss the LDA model and inference using this model. For further refernece, David Blei has an [excellent couple of talks](http://videolectures.net/mlss09uk_blei_tm/) introducing topic modelling, and LDA in particular.\n",
    "\n",
    "### Probabalistic generative model\n",
    "\n",
    "LDA assumes that there are some number k of topics that live outside of the document collection. Each topic is a distribution over the entire fixed vocabulary of v words (that is, a vector that assigns a probability to every word in teh vocabulary). The topics (the distributions over words) follow a v-dimensinoal Dirichlet distribution : $\\beta_k ~ Dir(\\eta)$. \n",
    "\n",
    "*Aside:* If you haven't seen it before, don't be confused by the fact that the Dirichlet is a distribution of distributions. Each draw from the Dirichlet assigns a probability to each element of a vector (whose length is determined by the dimensinoality of the Dirichlet), and these vectors of probabilities (which in our case we call topics) follow a Dichlet distribution. \n",
    "\n",
    "LDA posits the following data generation process:\n",
    "\n",
    "For each document, d\n",
    "<ol>\n",
    "<li>Draw a distribution over all k topics from the Dirichlet distribution: $\\theta_d ~ Dir(\\alpha)$. (Again, each individual draw is a vector of probabilities over all k topics, and these vectors follow a Dirichlet distribution). \n",
    "<li>For each word, n, in the document:\n",
    "<ol>\n",
    "<li>Draw a topic assignment from a multinomial distribution with the parameters you drew in (1): $Z_{d,n} ~ Multi(\\theta_d)$. Use this to select the corresponding topic, \\beta_{z_{dn}}, from the top matrix.\n",
    "<li>Draw a word from the topic that you selected in (2i): \n",
    "</ol>\n",
    "</ol>\n",
    "\n",
    "### Computation (inferring hidden variables from the data)\n",
    "\n",
    "The only variable form the above model that we actually get to observe is the words W_{d,n}. We need to infer the latent (hidden) variables in the model from the data; specifically, we need to infer:\n",
    "<li>per-word topic assignments: $z_{d,n}$</li>\n",
    "<li>per-document topic proportions: $\\theta_d$</li>\n",
    "<li>per-corpus topic distributions: $\\beta_k$</li>\n",
    "\n",
    "LDA is a hierarchical Bayesian model: a statistical model written in multiple levels that estimates the parameters of the posterior distribution using the Bayesian method. In this case, the posterior is the distribution of the hidden variables given the observations (words). \n",
    "\n",
    "Unfortunately, this posterior is intractable to compute. Therefore, we appeal to approximate posterior inference of the posterior (that is, we need a method that will infer an approximation of the posterior distribution). There are a variety of methods for doing this:\n",
    "<li>Gibbs sampling</li>\n",
    "<li>variational methods</li>\n",
    "<li>per-corpus topic distributions: $\\beta_k$</li>\n",
    "\n",
    "### Setting up our model and algorithm\n",
    "\n",
    "There a number of things that we may want to explore varying in setting up our model (this list is not exhaustive, for example, we might also explore which n-grams to include in our bag of words):\n",
    "<ul>\n",
    "<li>number of topics, k\n",
    "<li>model hyperparameters (paramaters of the prior distributions)\n",
    "<ul>\n",
    "<li>$\\alpha$ - influences document-topic density: with a higher alpha documents are like to be made up of a mixture of most of the topics, and not any single topic specifically. A low alpha value puts less such constraints on documents and means that it is more likely that a document may contain mixture of just a few, or even only one, of the topics. To visualize this, assume a symetric distribution (only one value for alpha) and that there are three topics (so alpha is of length three, and each draw from the Dirichlet is a vecotr of length three). If alpha is greater than 1 there will be a peak to the distribution over vectors centered at $E[\\theta_i|\\alpha]$. The higher the value of alpha, the more peaky this distribution, that is, the more probability each draw assigns to vectors near the mean, and the less to other vectors (notably, vectors in the conresr that put a lot of weight on just one topic. \n",
    "<li>$\\eta$ - similarly influence topic-word density: a high beta-value means that each topic is likely to contain a mixture of most of the words, and not any word specifically, while a low value means that a topic may contain a mixture of just a few of the words..\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "In addition, we could experiment with different algorithms in training our LDA model.\n",
    "\n",
    "## Load earlier list of text from websites\n",
    "\n",
    "Note that this is the data prior to \"preprocessing\" (although I have already cleaned it up a bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blist_1 = blist_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## If you want to write this list to txt\n",
    "# ## (ie dependigng on what you want to do here and what on EC2)\n",
    "# with open(\"blurb_list_first_200.txt\", \"wb\") as f:\n",
    "#     f.write(\"\\n\".join(map(str, blist_1)))\n",
    "# ## NB. if you use .csv as extension, loading the file in Excel will separate the strings after each comma.\n",
    "# ## This is problematic, as there are commans in the text strings in my dataset\n",
    "\n",
    "# ##And to then read the list back in\n",
    "# with open(\"blurb_list_first_200.txt\", \"r\") as f_1:\n",
    "#     content = f_1.readlines()\n",
    "    \n",
    "# blist_1_reconstructed = [x.strip() for x in content]\n",
    "# print len(blist_1[0])\n",
    "# print len(blist_1_reconstructed[0])\n",
    "\n",
    "# for i in range(len(blist_1)):\n",
    "#     if blist_1[i] != blist_1_reconstructed[i]:\n",
    "#         print i\n",
    "\n",
    "# print blist_1[106]               ##there is an issue with one of the strings not matching, but I'm not sure why\n",
    "# print blist_1_reconstructed[106]\n",
    "\n",
    "# blist_1_reconstructed == blist_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "As with k-means, I first tokenize, remove stopwords, and stem, make all letters lower case and drop punctuation, and make sure each string/token contains letters and is longer than two characters.\n",
    "\n",
    "However, I now include a set of custom stopwords. These are stopwords that, based on looking at the top words in each topic, I felt that it was not helpful to include. For example,....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'welcom', u'thoma', u'coram', u'centr', u'campus', u'centr', u'situat', u'special', u'place', u'children', u'sinc', u'middl', u'eighteenth', u'centuri', u'captain', u'coram', u'open', u'found', u'ling', u'hospit', u'present', u'centr', u'form', u'bring', u'togeth', u'leonard', u'nurseri', u'school', u'coram', u'communiti', u'nurseri', u'open', u'septemb']\n",
      "[u'citi', u'london', u'school', u'girl', u'welcom', u'pupil', u'wide', u'rang', u'differ', u'cultur', u'faith', u'background', u'creat', u'uniqu', u'dynam', u'learn', u'environ', u'school', u'commit', u'provid', u'opportun', u'girl', u'regardless', u'financi', u'mean', u'therefor', u'offer', u'numer', u'scholarship', u'bursari', u'citi', u'london', u'school', u'girl', u'aim', u'provid', u'stretch', u'challeng', u'academ', u'educ', u'girl', u'top', u'end', u'abil', u'rang', u'school', u'also', u'aim', u'provid', u'full', u'round', u'educ', u'help', u'develop', u'pupil', u'moral', u'spiritu', u'social', u'cultur', u'well', u'intellectu']\n",
      "[u'welcom', u'beckford', u'primari', u'school', u'websit', u'beckford', u'motto', u'togeth', u'achiev', u'children', u'beckford', u'exemplifi', u'school', u'motto', u'show', u'time', u'care', u'support', u'read']\n",
      "[u'welcom', u'camden', u'centr', u'learn', u'websit', u'camden', u'centr', u'learn', u'commit', u'provid', u'highest', u'qualiti', u'provis', u'learn', u'experi', u'young', u'peopl', u'passion', u'support', u'student', u'reengag', u'learn', u'fulfil', u'potenti', u'provid', u'personalis', u'learn', u'opportun', u'broad', u'flexibl', u'rang', u'curriculum', u'pathway', u'ensur', u'student', u'achiev', u'success', u'team', u'profession', u'experienc', u'dedic', u'staff', u'strive', u'relentless', u'ensur', u'provid', u'best', u'possibl', u'opportun', u'young', u'peopl', u'attend', u'gain', u'highest', u'level', u'achiev', u'person', u'develop', u'integr', u'multiag', u'team', u'provid', u'coher', u'respons', u'approach', u'meet', u'holist', u'need', u'young', u'peopl', u'famili', u'believ', u'individu', u'student', u'famili', u'individu', u'deserv', u'treat', u'respect', u'digniti', u'jeanett', u'low', u'director', u'camden', u'centr', u'learn']\n",
      "[u'give', u'great', u'pleasur', u'welcom', u'school', u'trust', u'enjoy', u'explor', u'websit', u'discov', u'believ', u'brookfield', u'primari', u'school', u'excit', u'vibrant', u'place', u'learn', u'would', u'like', u'inform', u'school', u'want', u'arrang', u'visit', u'pleas', u'hesit', u'contact', u'via', u'follow', u'link', u'telephon', u'mark', u'stub', u'want', u'top', u'tip', u'read', u'child', u'check', u'video', u'section', u'news', u'event']\n"
     ]
    }
   ],
   "source": [
    "nltk_stoplist = nltk.corpus.stopwords.words('english')\n",
    "custom_stoplist = []  # words I decided to drop based on reviewing model output \n",
    "stoplist = nltk_stoplist + custom_stoplist\n",
    "\n",
    "prepped_list = []\n",
    "for b in blist_1:\n",
    "    prepped_list.append(prep_blurb(b)) \n",
    "\n",
    "for i in range(5):\n",
    "    print prepped_list[i]\n",
    "    \n",
    "##!!! I should double check that prep_blurb is in fact using a different stoplist. To do this,\n",
    "## make stoplist point to a string or something else that should break the function!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I next turn my tokenized, stemmed etc. blurbs into an id-term dictionary. The Dictionary() function traverses texts, assigning a unique integer id to each unique token while also collecting word counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(prepped_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then remove infrequent and frequent words by using the dictionary.filter_extremes() method. I remove all words that appear in at least 5 documents, removed all words that appeared in more than 60% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 194, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 484, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'consider'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=0.6, keep_n=None)\n",
    "print dictionary.keys()[:30]\n",
    "dictionary[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The doc2bow() function converts dictionary into a bag-of-words. The result, corpus, is a list of vectors equal to the number of documents. In each document vector is a series of tuples: (term ID, term frequency). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308\n",
      "[(0, 2), (188, 1), (201, 1), (213, 1), (269, 1), (294, 1), (321, 1), (374, 1), (380, 1), (389, 1), (392, 1), (404, 3), (409, 2), (497, 1), (530, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in prepped_list]\n",
    "print len(corpus)\n",
    "print corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to run my LDA models on EC2. I therefore need to save my dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary.save('lda_dictionary.dict')\n",
    "corpora.MmCorpus.serialize('lda_corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that I am able to load the dictionary and corpus without introducing changes from the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 194, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 484, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "consider\n",
      "[(0, 2.0), (188, 1.0), (201, 1.0), (213, 1.0), (269, 1.0), (294, 1.0), (321, 1.0), (374, 1.0), (380, 1.0), (389, 1.0), (392, 1.0), (404, 3.0), (409, 2.0), (497, 1.0), (530, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "test_dictionary = dictionary.load('lda_dictionary.dict')\n",
    "print dictionary.keys()[:30]\n",
    "print dictionary[1]\n",
    "\n",
    "test_corpus = corpora.MmCorpus('lda_corpus.mm')\n",
    "print test_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using EC2 \n",
    "\n",
    "I now want to go ahead and fit my first LDA model on  on EC2.  **give details of setting up EC2 and terminal commands for running a file - then below I can just say that I ran a cel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit initial LDA model\n",
    "\n",
    "I fit my LDA model using gensim, which uses uses [online variational Bayes](https://rare-technologies.com/tutorial-on-mallet-in-python/). Parameters used below:\n",
    "<ul>\n",
    "<li>num_topics: *required.* An LDA model requires the user to determine how many topics should be generated. Our document set is small, so were only asking for three topics.\n",
    "<li>id2word: *required.* The LdaModel class requires our previous dictionary to map ids to strings.\n",
    "<li>passes: *optional.* The number of laps the model will take through corpus. The greater the number of passes, the more accurate the model will be. A lot of passes can be slow on a very large corpus.\n",
    "</ul>\n",
    "\n",
    "I save the following cell as ??? and run it on EC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025*\"capac\" + 0.022*\"meet\" + 0.020*\"strong\" + 0.014*\"email\" + 0.013*\"perform\"\n",
      "0.020*\"happen\" + 0.020*\"websit\" + 0.018*\"dont\" + 0.017*\"maintain\" + 0.017*\"contribut\"\n",
      "0.028*\"pupil\" + 0.024*\"meet\" + 0.020*\"project\" + 0.015*\"staff\" + 0.014*\"fulli\"\n"
     ]
    }
   ],
   "source": [
    "# Read in dictionary and corpus\n",
    "test_dictionary = dictionary.load('lda_dictionary.dict')\n",
    "test_corpus = corpora.MmCorpus('lda_corpus.mm')\n",
    "\n",
    "# Fit model\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "\n",
    "# With LDA, a topic is a probability distribution over words in the vocabulary; that is, each \n",
    "# topic assigns a particular probability to every one of the unique words that appears in our \n",
    "# data. Different topics will assign different probabilities to the same word. \n",
    "# Next, therefore, let's look at the highest probability words in each topic will thus give \n",
    "# us a sense of its major themes.\n",
    "topic_list = lda.print_topics(num_topics=3, num_words=5)\n",
    "for t in topic_list:\n",
    "    print t[1]\n",
    "    \n",
    "# Take a look at the topic proportions for each blurb \n",
    "# for row in range(len(corpus)): \n",
    "#     print lda[corpus[row]]\n",
    "\n",
    "# Convert topic proportions for each blurb (gensim.interfaces.TransformedCorpus object)\n",
    "# to list of lists (nb. length of topic proportion lists may vary across topics, because \n",
    "# doesn't report very small proportions)\n",
    "results_list = []\n",
    "for row in range(len(corpus)):\n",
    "    results_list.append(lda[corpus[row]])\n",
    "\n",
    "# Write list of lists to json\n",
    "# with open('lda_blurb_topics.txt','w') as myfile:\n",
    "#     json.dump(results_list, myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed parallel processing with gensim on EC2\n",
    "\n",
    "\n",
    "\n",
    "## Choose the number of topics\n",
    "\n",
    "To compare the performance of models with different parameter values I use per-word perpelexity, folowing [Blie et al. (2003)](http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf). Perplexity is a measure of the likelihood achieved on a held-out test set. A lower perplexity indicates better performance. Whilst perplexity is the most appropriate measure that I know of for comparing LDA models, it sits somewhat uncomfortably in this context. Perplexity is a measure of predictive performance, but I am not using LDA for the purposes of prediction.\n",
    "\n",
    "I being by calculating the perplexity of models with different numbers of topics. I save the following cell as ??? and run it on EC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set the different numbers of topics that I will try\n",
    "num_topics = [3,4]\n",
    "\n",
    "# Read in dictionary and corpus\n",
    "test_dictionary = dictionary.load('lda_dictionary.dict')\n",
    "test_corpus = corpora.MmCorpus('lda_corpus.mm')\n",
    "\n",
    "# Compute perplexities for different numbers of topics\n",
    "num_topic_perplexity = function(num_topic_list){\n",
    "    # shuffle corpus\n",
    "    cp = list(corpus)\n",
    "    random.shuffle(cp)\n",
    "\n",
    "    # split into 80% training and 20% test sets\n",
    "    p = int(len(cp) * .8)\n",
    "    cp_train = cp[0:p]\n",
    "    cp_test = cp[p:]\n",
    "\n",
    "    perplexities = []\n",
    "    for n in num_topic_list:\n",
    "        # fit model\n",
    "        lda = gensim.models.ldamodel.LdaModel(cp_train, num_topics=n, id2word=dictionary, passes=20)\n",
    "\n",
    "        # calculate per-word perplexity\n",
    "        per_word_perplex = np.exp2(-lda.log_perplexity(cp_test))\n",
    "        # Alternative way of getting per-word perplexity\n",
    "        # per_word_perplex = np.exp2(-lda.bound(cp_test) / sum(cnt for document in cp_test for _, cnt in document))\n",
    "\n",
    "        perplexities.append(per_word_perplex)\n",
    "}\n",
    "topics_perps_lists = [num_topics, num_topic_perplexity] \n",
    "\n",
    "## Write lists of number of topics and corresponding perplexities to json\n",
    "with open('topics_perps_lists.txt','w') as myfile:\n",
    "    json.dump(topics_perps_lists, myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and plot these results locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that per-word hold-out perplexity is decreasing in the number of topics. Again, I look for the elbow, which appears around ???. I choose ??? as the number of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the values of alpha and eta\n",
    "\n",
    "I next conduct a grid search to explore how perplexity varies with different values of alpha and eta. I run the following cell in EC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[1, 2]\n",
      "[1, 3]\n",
      "[1, 4]\n",
      "[2, 1]\n",
      "[2, 2]\n",
      "[2, 3]\n",
      "[2, 4]\n",
      "[3, 1]\n",
      "[3, 2]\n",
      "[3, 3]\n",
      "[3, 4]\n",
      "[4, 1]\n",
      "[4, 2]\n",
      "[4, 3]\n",
      "[4, 4]\n"
     ]
    }
   ],
   "source": [
    "## Set the parameter values that I will search over\n",
    "param_values = [0.001, 0.005, 0.01]\n",
    "\n",
    "# Read in dictionary and corpus\n",
    "test_dictionary = dictionary.load('lda_dictionary.dict')\n",
    "test_corpus = corpora.MmCorpus('lda_corpus.mm')\n",
    "\n",
    "# Compute grid of perplexities for different values of alpha and beta\n",
    "alpha_eta_perplexity = function(param_list){\n",
    "    ##Returns grid of perplexities with alpha on x axis, and eta on y axis\n",
    "    # shuffle corpus\n",
    "    cp = list(corpus)\n",
    "    random.shuffle(cp)\n",
    "\n",
    "    # split into 80% training and 20% test sets\n",
    "    p = int(len(cp) * .8)\n",
    "    cp_train = cp[0:p]\n",
    "    cp_test = cp[p:]\n",
    "\n",
    "    # compute perplexities\n",
    "    grid_vals = {}\n",
    "    for alpha_val in param_list:\n",
    "        perp_vals = []\n",
    "        for eta_val in param_list:\n",
    "            lda = gensim.models.ldamodel.LdaModel(cp_train, num_topics=n, alpha=alpha_val, eta=eta_val, id2word=dictionary, passes=20)\n",
    "            per_word_perplex = np.exp2(-lda.log_perplexity(cp_test))\n",
    "            perp_vals.append(per_word_perplex)\n",
    "        grid_vals[alpha_var] = perp_vals\n",
    "    \n",
    "    # conver dictionary to DataFrame\n",
    "    final_grid = pd.DataFrame(grid_vals)\n",
    "    final_grid.index = param_list\n",
    "    return final_grid\n",
    "}\n",
    "\n",
    "alpha_eta_grid = alpha_eta_perplexity(param_values)\n",
    "\n",
    "# Write grid of perplexities to csv\n",
    "alpha_eta_grid.to_csv('alpha_eta_grid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in this grid locally and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default for both alpha and eta is a 1/num_topics symetric prior, and in this case 1/num_topics=???. [discuss which value I will choose - hopefully can just use defaults]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model with chosen parameters\n",
    "\n",
    "Need to return top words for each topics and topic proportions for each blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "First, I should like to systematically find the best number of topics, k, to use, and as well best values to use for the hyperparameters alpha and eta. In order to do this I will first need to define \"best\", and perplexity looks like a good option. It should be possible to calculate perplexity using a [Python wrapper](https://github.com/clips/topbox) for the [Stanford Topic Modeling Toolbox](http://nlp.stanford.edu/software/tmt/tmt-0.4/). I can then conduct grid searches for the best values of k, alpha, and eta. In order to do this, I will likely need to use the [parallel processing tools build into gensim](https://radimrehurek.com/gensim/dist_lda.html), together with a service such as AWS. My earlier project with GreatSchools data walks through doing this.\n",
    "\n",
    "Second, so far in training my LDA model I have only used [\"online variational Bayes\"](https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf), the [method built into gensim](https://rare-technologies.com/multicore-lda-in-python-from-over-night-to-over-lunch/). But Gibbs sampling has advantages over this method. I should therefore like to instead try repeating my analsis with Gibbs sampling. I could perhaps again use perplexity to compare outcomes from my analysis with Gibbs sampling to my analysis with variation inference. There is a wrapper in Python that should allow me to use Gibbs sampling with Gensim; however, it might instead be easier to use the LDA method in graphlab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine k-means and LDA results with school characteristics data\n",
    "\n",
    "I next join the blurbs and classifications with data on school characteristics from EduBase]. **Be sure to add link to school performance data, and ideally also look at relationship to survey data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 4)\n",
      "        URN                                             blurbs  km_assignment  \\\n",
      "0  119460.0  Welcome to Adlington St Paul's Church of Engla...              1   \n",
      "1  139709.0  Welcome to Abbey Woods Academy Abbey Woods is ...              1   \n",
      "2  114864.0  We aim to make school a happy and rewarding ex...              3   \n",
      "3  131982.0  Menu Welcome from the Principal Principals Blo...              2   \n",
      "4  121326.0  Welcome to Alanbrooke Community Primary School...              3   \n",
      "\n",
      "                school_type  \n",
      "0    Voluntary Aided School  \n",
      "1       Academy Sponsor Led  \n",
      "2          Community School  \n",
      "3  Other Independent School  \n",
      "4          Community School  \n"
     ]
    }
   ],
   "source": [
    "# Load school characteristics data\n",
    "school_dat = pd.read_csv('edubasealldata.csv', low_memory=False)\n",
    "# print school_dat.head()\n",
    "school_dat = school_dat[['URN','TypeOfEstablishment (name)']]\n",
    "# print school_dat.head()\n",
    "\n",
    "#Load school performance data\n",
    "# ks4 = pd.read_csv('Performancetables_1516/england_ks4revised.csv', low_memory=False)\n",
    "# ks4 = ks4[['URN', 'ATT8SCR']]\n",
    "# print ks4.head()\n",
    "\n",
    "# Merge kmeans and school characteristics datasets\n",
    "df = pd.merge(classifed_blurbs, school_dat, how='left', on='URN')\n",
    "print df.shape\n",
    "\n",
    "# Then merge with school performance dataset\n",
    "# df = pd.merge(df, ks4, how='left', on='URN') # getting lots of obs - must be repeated URNs in ks4\n",
    "# print df.shape\n",
    "\n",
    "# Rename columns\n",
    "df.columns = ['school_type' if x=='TypeOfEstablishment (name)' else x for x in df.columns]\n",
    "\n",
    "print df.head()\n",
    "\n",
    "# Save to csv\n",
    "\n",
    "# Free up memory\n",
    "del school_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze relationship between website text and school characteristics\n",
    "\n",
    "### Plot proportion in each group by school type\n",
    "\n",
    "I may want to use the stacked bar graph package here: https://github.com/minillinim/stackedBarGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voluntary Aided School 6.0\n",
      "Academy Sponsor Led 6.5\n",
      "Community School 28.0\n",
      "Other Independent School 3.0\n",
      "no_type 27.5\n",
      "Academy Converter 14.0\n",
      "Foundation School 5.0\n",
      "Voluntary Controlled School 4.5\n",
      "Academy Special Converter 1.5\n",
      "Community Special School 1.0\n",
      "LA Nursery School 1.5\n",
      "Free Schools 0.5\n",
      "Pupil Referral Unit 0.5\n",
      "Free Schools - Alternative Provision 0.5\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "df.school_type.fillna('no_type', inplace=True)\n",
    "for t in df['school_type'].unique():\n",
    "    print t, float(len(df[df['school_type']==t]))*100.0/float(len(df))\n",
    "    tot += float(len(df[df['school_type']==t]))*100.0/float(len(df))\n",
    "print tot\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my k-means analysis I would think a multinomial model would be good for exploring, for example, differences across groups or with characteristics in the proportion falling in differnet categories. For the LDA analysis, I will have proportions summing to one for each variable. I will have to investigate the best model for this - but it could well be something again involving a Dirichlet. \n",
    "\n",
    "Of course, any such analysis won't take account of the uncertainty in my paramater estimtes, and I should note this limitation and that I like likely need to explore Bayesian methods to address it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
