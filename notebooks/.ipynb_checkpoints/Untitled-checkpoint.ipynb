{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions and stemmer to preprocess data\n",
    "\n",
    "For both my k-means and LDA analysis below I need to first (in the following order):\n",
    "<ul>\n",
    "<li>**Tokenize**: divide string into a list of substrings </li>\n",
    "<li>**Remove stopwords**: stopwords are a list of high frequency words like, the, to, and also</li>\n",
    "<li>**Stem**: take the root of each word</li>\n",
    "</ul>\n",
    "\n",
    "In addition, I make all letters lower case and drop punctuation, and make sure each string/token contains letters and is longer than two characters. I therefore create the following functions in preprocess_functions.py, which I read in when doing by analysis for both k-means and LDA.\n",
    "\n",
    "```Python\n",
    "## .translate only takes str, whereas TfidfVectorizer only takes unicode.\n",
    "## Therefore, to use .translate in the tokenizer in TfidfVectorizer I need to\n",
    "## write a function that converts unicode -> string, applies .translate,\n",
    "## and then converts it back\n",
    "def no_punctuation_unicode(text):\n",
    "    str_text = str(text)\n",
    "    no_punctuation = str_text.translate(None, string.punctuation)\n",
    "    unicode_text = no_punctuation.decode('utf-8')\n",
    "    return unicode_text\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def prep_blurb(text):\n",
    "    lowers = text.lower()\n",
    "    no_punct = no_punctuation_unicode(lowers)\n",
    "    tokens = nltk.word_tokenize(no_punct)\n",
    "    has_letters = [t for t in tokens if re.search('[a-zA-Z]',t)]\n",
    "    no_numbers  = [t for t in has_letters if not hasNumbers(t)]\n",
    "    drop_stops = [w for w in no_numbers if not w in stoplist] \n",
    "    stems = [stemmer.stem(t) for t in drop_stops]\n",
    "    drop_short = [s for s in stems if len(s)>2]  ##not sure about this line\n",
    "    return drop_short\n",
    "\n",
    "## Get stemmer outside of prep_blurb() function, so don't have to re-do every time I use the function\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From 3_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## If you want to write list blurbs to txt\n",
    "# ## (ie dependigng on what you want to do here and what on EC2)\n",
    "# with open(\"blurb_list_first_200.txt\", \"wb\") as f:\n",
    "#     f.write(\"\\n\".join(map(str, blist_1)))\n",
    "# ## NB. if you use .csv as extension, loading the file in Excel will separate the strings after each comma.\n",
    "# ## This is problematic, as there are commans in the text strings in my dataset\n",
    "\n",
    "# ##And to then read the list back in\n",
    "# with open(\"blurb_list_first_200.txt\", \"r\") as f_1:\n",
    "#     content = f_1.readlines()\n",
    "    \n",
    "# blist_1_reconstructed = [x.strip() for x in content]\n",
    "# print len(blist_1[0])\n",
    "# print len(blist_1_reconstructed[0])\n",
    "\n",
    "# for i in range(len(blist_1)):\n",
    "#     if blist_1[i] != blist_1_reconstructed[i]:\n",
    "#         print i\n",
    "\n",
    "# print blist_1[106]               ##there is an issue with one of the strings not matching, but I'm not sure why\n",
    "# print blist_1_reconstructed[106]\n",
    "\n",
    "# blist_1_reconstructed == blist_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried defining the functions below in a separate file, as I use them both here and in my k-means analysis. However,\n",
    "if I define prep_blurb in a separate file, it is unable to find stoplist (and I have tried defining stoplist as global in different places, and changing where I import the function). The problem is that ***a function uses the globals of the modules it's defined in***."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
