{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent dirichlet allocation (LDA)\n",
    "\n",
    "I identify the topics discussed in each school webside blurb using Latent dirichlet allocation (LDA). LDA is a mixed membership model. Mixed memership models allow us to associate a given data point with a set of different cluster assignments and to capture the relative proportion of different clusters in a datapoint.\n",
    "\n",
    "## Load packages and data\n",
    "\n",
    "Read in packages and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> averaged_perceptron_tagger\n",
      "    Downloading package averaged_perceptron_tagger to\n",
      "        /home/ec2-user/nltk_data...\n",
      "      Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "from py_files.importing_packages import *\n",
    "import random\n",
    "import gensim\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download()   # modules I (possibly) need are averaged_perceptro n_tagger, \n",
    "# maxent_treebank_pos_tagger, punkt, stopwords, wordnet \n",
    "# (running locally on my mac, will open a new window (you need to close it before can continue)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4676\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../my_datasets/prepped.csv')\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of URNs (will need later when put it back together) and blurbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urn_series = df[\"urn\"]\n",
    "blist = df[\"blurb\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Before fitting my LDA model, I need to break each blurb into individual elements that can serve as features. In addition to tokenizing (breaking up each blurb) I will perform a number of steps, in the following order (although note that there are multiple ways in which these operations could be combined):\n",
    "<ol>\n",
    "<li>**Remove all non-ASCII characters**: eg. \\xe2\\x80\\x98</li>\n",
    "<li>**Make all words lowercase**</li>\n",
    "<li>**Remove punctuation**</li>\n",
    "<li>**Tokenize**: divide string into a list of substrings.</li>\n",
    "<li>**Remove words not containing letters**</li>\n",
    "<li>**Remove words containing numbers**</li>\n",
    "<li>**Remove short words**: that is, of 3 characters or less.</li>\n",
    "<li>**Remove NLTK stopwords**: stopwords are a list of high frequency words like, the, to, and also.</li>\n",
    "<li>**Get parts of speech**: performed using [NLTK’s Treebank maximum entropy classifier](http://nltk.org/api/nltk.classify.html) based pos tagger. This is a slow but accurate pos tagger that was originally trained on the [Treebank](http://en.wikipedia.org/wiki/Treebank) corpus. Ideally, we would have used a classifier that was trained on our school website text, but I decided to take this faster approach as a first cut. Most topic models are built using nouns as they are most representative of concrete topics. Although adjectives, verbs and other parts of speech can add interesting context, they generally overpower the underlying nouns and can inject sentiment.</li>\n",
    "<li>**Lemmatize**: using [NLTK’s WordNet Lemmatizer](http://nltk.org/_modules/nltk/stem/wordnet.html). This reduces the dimensions of the data even further by aggregating words that either are the same root or have the same meaning.  The lemmatizer uses the pos tags to understand what words might be synonyms.  The lemmatizer would take a word such as *teacher* and *teachers* and make sure both were just *teacher*. I used lematization rather than stemming, as stemmers can aggressively mangle words.</li>\n",
    "<li>**Remove custom stopwords**: I drop a set of custom stopwords, that is, stopwords that, based on looking at the top words in each topic, I think that it was not helpful to include.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove all non-ASCII characters from blurbs (eg. \\xe2\\x80\\x98)\n",
    "new_list = []\n",
    "for b in blist:\n",
    "    new_list.append(b.decode('utf8').encode('ascii', errors='ignore'))\n",
    "blist = new_list\n",
    "\n",
    "## Create lists of stopwords \n",
    "nltk_stoplist = nltk.corpus.stopwords.words('english')\n",
    "custom_stoplist = []\n",
    "# custom_stoplist = ['child', 'children', 'pupil', 'student', 'princip', 'headteach',\n",
    "#                    'parent', 'staff', 'peopl','governor','head','teacher',\n",
    "#                    'girl','boy', 'age','young', \n",
    "#                    'communiti', 'free', 'primari', 'sixth', 'colleg', 'england',\n",
    "#                    'george','king','infant','nurseri','junior',\n",
    "#                    'websit', 'pleas','thank','interest', 'link', 'like', 'page', 'click', \n",
    "#                    'find', 'contact','inform', 'look', 'news', 'newslett','visit',\n",
    "#                    'form', 'twitter','site','telephon','message','email','updat','blog',\n",
    "#                    'septemb', 'februari','march','week', 'year','term', 'date','time','monday',\n",
    "#                    'would', 'everi', 'make', 'look', 'come', 'well', 'area',\n",
    "#                    'togeth', 'need', 'give', 'also', 'use', 'offic', 'keep',\n",
    "#                    'know', 'posit','rang','feder', 'around','part','follow',\n",
    "#                    'take','includ','nation',\n",
    "#                    'provid', 'build', 'offer', 'work', 'educ', 'learn',\n",
    "#                    'event','polici','class','hope','best','good','high',\n",
    "#                    'cathol', 'church', 'villag','lot','back',\n",
    "#                    'copi','view','detail','life']  \n",
    "\n",
    "def no_punctuation_unicode(text):\n",
    "    # remove puncuation from text\n",
    "    '''converts unicode -> string, applies .translate,\n",
    "    and then converts it back'''\n",
    "    str_text = str(text)\n",
    "    no_punctuation = str_text.translate(None, string.punctuation)\n",
    "    unicode_text = no_punctuation.decode('utf-8')\n",
    "    return unicode_text\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    # check if inputString contains numbers   \n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "# Instantiate a WordNet Lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "        \n",
    "# Will need to convert penn_tag to morphy_tag used by WordNet Lemmatizer\n",
    "# http://stackoverflow.com/questions/5364493/lemmatizing-pos-tagged-words-with-nltk\n",
    "morphy_tag = {'NN':wordnet.NOUN,'JJ':wordnet.ADJ,'VB':wordnet.VERB,'RB':wordnet.ADV}\n",
    "        \n",
    "def prep_blurb(blurb):\n",
    "    blurb = blurb.lower()  # make lowercase\n",
    "    blurb = no_punctuation_unicode(blurb) # remove punctuation\n",
    "    blurb = nltk.word_tokenize(blurb) # tokenize  \n",
    "    blurb = [t for t in blurb if re.search('[a-zA-Z]',t)] # remove words not containing letters\n",
    "    blurb = [t for t in blurb if not hasNumbers(t)] # remove words containing numbers\n",
    "    blurb = [t for t in blurb if len(t)>3]  # remove short words\n",
    "    blurb = [t for t in blurb if not t in nltk_stoplist]  #remove nltk stopwords\n",
    "    blurb = [word for word in nltk.pos_tag(blurb)]  # get parts of speech\n",
    "    blurb = [word for word in blurb if word[1] in ['NN','NNS']]  # reduce to only nouns\n",
    "    blurb = [wnl.lemmatize(word[0],morphy_tag[word[1][:2]]) for word in blurb]  # lemmatize\n",
    "    blurb = [t for t in blurb if len(t)>3]  # remove short words a second time in case introduced by lemmatization\n",
    "    blurb = [t for t in blurb if not t in nltk_stoplist] # remove nltk stopwords a second time in case introduced by lemmatization\n",
    "    blurb = [t for t in blurb if not t in custom_stoplist]  # remove custom stopwords \n",
    "    return blurb\n",
    "\n",
    "prepped_list = []\n",
    "for b in blist:\n",
    "    prepped_list.append(prep_blurb(b)) \n",
    "\n",
    "# for i in range(5):\n",
    "#     print prepped_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I next turn my tokenized, stemmed etc. blurbs into an id-term dictionary. The Dictionary() function traverses texts, assigning a unique integer id to each unique token while also collecting word counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = corpora.Dictionary(prepped_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then remove infrequent and frequent words by using the dictionary.filter_extremes() method. I remove all words that appear in at least 5 documents, removed all words that appeared in more than 60% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8466\n",
      "1729\n",
      "[0, 1, 2, 3, 4, 5, 871, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 875, 25, 26, 27, 28, 29]\n",
      "similarity\n"
     ]
    }
   ],
   "source": [
    "print len(my_dict)\n",
    "my_dict.filter_extremes(no_below=5, no_above=0.6, keep_n=None)\n",
    "print len(my_dict)\n",
    "print my_dict.keys()[:30]\n",
    "print my_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The doc2bow() function converts dictionary into a bag-of-words. The result, corpus, is a list of vectors equal to the number of documents. In each document vector is a series of tuples: (term ID, term frequency). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4676\n",
      "[(27, 1), (119, 1), (221, 1), (292, 1), (397, 1), (400, 1), (782, 1), (841, 1), (888, 1), (921, 1), (1065, 1), (1133, 1), (1148, 1), (1167, 1), (1194, 1), (1307, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [my_dict.doc2bow(text) for text in prepped_list]\n",
    "print len(corpus)\n",
    "print corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to run my LDA models on EC2. I therefore need to save my dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict.save('../data/lda_dictionary.dict')\n",
    "corpora.MmCorpus.serialize('../data/lda_corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that I am able to load the dictionary and corpus without introducing changes from the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "similarity\n",
      "[(27, 1.0), (119, 1.0), (221, 1.0), (292, 1.0), (397, 1.0), (400, 1.0), (782, 1.0), (841, 1.0), (888, 1.0), (921, 1.0), (1065, 1.0), (1133, 1.0), (1148, 1.0), (1167, 1.0), (1194, 1.0), (1307, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "test_dictionary = corpora.Dictionary.load('../data/lda_dictionary.dict')\n",
    "print test_dictionary.keys()[:30]\n",
    "print test_dictionary[1]\n",
    "\n",
    "test_corpus = corpora.MmCorpus('../data/lda_corpus.mm')\n",
    "print test_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief overview of LDA\n",
    "\n",
    "Here, I briefly discuss the LDA model and inference using this model. For further details, David Blei has an [excellent couple of talks](http://videolectures.net/mlss09uk_blei_tm/) introducing topic modelling, and LDA in particular.\n",
    "\n",
    "### Probabalistic generative model\n",
    "\n",
    "LDA assumes that there are some number k of topics that live outside of the document collection. Each topic is a distribution over the entire fixed vocabulary of v words (that is, a vector that assigns a probability to every word in teh vocabulary). The topics (the distributions over words) follow a v-dimensional Dirichlet distribution : $\\beta_k \\sim Dir(\\eta)$. \n",
    "\n",
    "If you haven't seen it before, don't be confused by the fact that the Dirichlet is a distribution of distributions. Each draw from the Dirichlet assigns a probability to each element of a vector (whose length is determined by the dimensinoality of the Dirichlet), and these vectors of probabilities (which in our case we call topics) follow a Dichlet distribution. \n",
    "\n",
    "LDA posits the following data generation process:\n",
    "\n",
    "For each document, d\n",
    "<ol>\n",
    "<li>Draw a distribution over all k topics from the Dirichlet distribution: $\\theta_d \\sim Dir(\\alpha)$. (Again, each individual draw is a vector of probabilities over all k topics, and these vectors follow a Dirichlet distribution). \n",
    "<li>For each word, n, in the document:\n",
    "<ol>\n",
    "<li>Draw a topic assignment from a multinomial distribution with the parameters you drew in (1): $Z_{d,n} \\sim Multi(\\theta_d).$ Use this to select the corresponding topic, $\\beta_{z_{d,n}}$, from the topic matrix.\n",
    "<li>Draw a word from the topic that you selected in (2a): $w_{d,n} \\sim \\beta_{z_{d,n}}$\n",
    "</ol>\n",
    "</ol>\n",
    "\n",
    "### Computation (inferring hidden variables from the data)\n",
    "\n",
    "The only variable form the above model that we actually get to observe is the words W_{d,n}. We need to infer the latent (hidden) variables in the model from the data; specifically, we need to infer:\n",
    "<li>per-word topic assignments: $z_{d,n}$</li>\n",
    "<li>per-document topic proportions: $\\theta_d$</li>\n",
    "<li>per-corpus topic distributions: $\\beta_k$</li>\n",
    "\n",
    "LDA is a hierarchical Bayesian model: a statistical model written in multiple levels that estimates the parameters of the posterior distribution using the Bayesian method. In this case, the posterior is the distribution of the hidden variables given the observations (words). \n",
    "\n",
    "Unfortunately, this posterior is intractable to compute. Therefore, we appeal to approximate posterior inference of the posterior (that is, we need a method that will infer an approximation of the posterior distribution). There are a variety of methods for doing this:\n",
    "<li>Gibbs sampling</li>\n",
    "<li>variational methods</li>\n",
    "<li>per-corpus topic distributions: $\\beta_k$</li>\n",
    "\n",
    "### Setting up our model and algorithm\n",
    "\n",
    "There a number of things that we may want to explore varying in setting up our model (this list is not exhaustive, for example, we might also explore which n-grams to include in our bag of words):\n",
    "<ul>\n",
    "<li>number of topics, k\n",
    "<li>model hyperparameters (paramaters of the prior distributions)\n",
    "<ul>\n",
    "<li>$\\alpha$ - influences document-topic density: with a higher alpha documents are like to be made up of a mixture of most of the topics, and not any single topic specifically. A low alpha value puts less such constraints on documents and means that it is more likely that a document may contain mixture of just a few, or even only one, of the topics. To visualize this, assume a symetric distribution (only one value for alpha) and that there are three topics (so alpha is of length three, and each draw from the Dirichlet is a vecotr of length three). If alpha is greater than 1 there will be a peak to the distribution over vectors centered at $E[\\theta_i|\\alpha]$. The higher the value of alpha, the more peaky this distribution, that is, the more probability each draw assigns to vectors near the mean, and the less to other vectors (notably, vectors in the conresr that put a lot of weight on just one topic. \n",
    "<li>$\\eta$ - similarly influence topic-word density: a high beta-value means that each topic is likely to contain a mixture of most of the words, and not any word specifically, while a low value means that a topic may contain a mixture of just a few of the words..\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "In addition, we could experiment with different algorithms in training our LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit initial LDA model\n",
    "\n",
    "I fit my LDA model using gensim, which implements LDA using [online variational Bayes](https://rare-technologies.com/tutorial-on-mallet-in-python/). I run gensim on EC2 using a m4.2xlarge instance type, and use gensim's [ldamulticore](https://radimrehurek.com/gensim/models/ldamulticore.html) to parellize the model training across the instance's eight cores.\n",
    "\n",
    "I include the following parameters in the LDA model:\n",
    "<ul>\n",
    "<li>num_topics: *required.* An LDA model requires the user to determine how many topics should be generated. Our document set is small, so we’re only asking for three topics.\n",
    "<li>id2word: *required.* The LdaModel class requires our previous dictionary to map ids to strings.\n",
    "<li>passes: *optional.* The number of laps the model will take through corpus. The greater the number of passes, the more accurate the model will be. A lot of passes can be slow on a very large corpus.\n",
    "<li>workers: the number of extra processes to use for parallelization. Uses all available cores by default (so don't actually include this - just go with the default).\n",
    "<li>chunksize: the number of documents to sent to each worker. It uses 2000 by default, so instead I send it int(float(len(my_corpus))/my_workers), where my_workers is the number of cores.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see output as code is running, uncomment line below\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s: %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "my_workers = 8\n",
    "\n",
    "data_directory = '../data/'\n",
    "my_dict = corpora.Dictionary.load(data_directory + 'lda_dictionary.dict')\n",
    "my_corpus = corpora.MmCorpus(data_directory + 'lda_corpus.mm')\n",
    "\n",
    "my_chunksize = int(float(len(my_corpus))/my_workers)\n",
    "my_lda = gensim.models.LdaMulticore(my_corpus, num_topics=6, id2word=my_dict, passes=20, workers=my_workers, chunksize=my_chunksize)\n",
    "my_lda.save('../results/first_lda.lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then read in the results and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056*\"year\" + 0.050*\"student\" + 0.027*\"principal\" + 0.020*\"college\" + 0.018*\"term\" + 0.017*\"academy\" + 0.016*\"result\" + 0.013*\"parent\"\n",
      "0.050*\"church\" + 0.036*\"community\" + 0.020*\"value\" + 0.018*\"england\" + 0.016*\"village\" + 0.016*\"life\" + 0.015*\"heart\" + 0.015*\"parent\"\n",
      "0.049*\"year\" + 0.025*\"class\" + 0.017*\"area\" + 0.017*\"parent\" + 0.015*\"curriculum\" + 0.015*\"information\" + 0.014*\"sport\" + 0.014*\"community\"\n",
      "0.037*\"community\" + 0.022*\"life\" + 0.021*\"information\" + 0.021*\"parent\" + 0.018*\"value\" + 0.018*\"visit\" + 0.018*\"staff\" + 0.017*\"website\"\n",
      "0.036*\"parent\" + 0.034*\"information\" + 0.030*\"please\" + 0.029*\"staff\" + 0.022*\"visit\" + 0.021*\"pupil\" + 0.018*\"hope\" + 0.018*\"community\"\n",
      "0.030*\"student\" + 0.027*\"community\" + 0.020*\"pupil\" + 0.020*\"staff\" + 0.019*\"education\" + 0.018*\"environment\" + 0.018*\"opportunity\" + 0.017*\"academy\"\n",
      "\n",
      "[(3, 0.51895881717116521), (4, 0.44153533345985507)]\n",
      "[(1, 0.77401782837958222), (2, 0.20107173686321486)]\n",
      "[(3, 0.14843251172365707), (5, 0.82912129652319422)]\n"
     ]
    }
   ],
   "source": [
    "test_lda = gensim.models.LdaModel.load('../results/first_lda.lda')\n",
    "\n",
    "# With LDA, a topic is a probability distribution over words in the vocabulary; that is, each \n",
    "# topic assigns a particular probability to every one of the unique words that appears in our \n",
    "# data. Different topics will assign different probabilities to the same word. \n",
    "# Next, therefore, let's look at the highest probability words in each topic, which will give \n",
    "# us a sense of its major themes.\n",
    "topic_list = test_lda.print_topics(num_topics=6, num_words=8)\n",
    "for t in topic_list:\n",
    "    print t[1]\n",
    "print\n",
    "    \n",
    "# Take a look at the topic proportions for each blurb \n",
    "for row in range(3): \n",
    "    print test_lda[corpus[row]]\n",
    "\n",
    "# Convert topic proportions for each blurb (gensim.interfaces.TransformedCorpus object)\n",
    "# to list of lists (nb. length of topic proportion lists may vary across topics, because \n",
    "# doesn't report very small proportions)\n",
    "# results_list = []\n",
    "# for row in range(len(corpus)):\n",
    "#     results_list.append(lda[corpus[row]])\n",
    "\n",
    "# Write list of lists to json\n",
    "# with open('lda_blurb_topics.txt','w') as myfile:\n",
    "#     json.dump(results_list, myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the number of topics\n",
    "\n",
    "To compare the performance of models with different parameter values I use per-word perpelexity, folowing [Blie et al. (2003)](http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf). Perplexity is a measure of the likelihood achieved on a held-out test set. A lower perplexity indicates better performance. \n",
    "\n",
    "I begin by calculating the perplexity of models with different numbers of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training set: 3740\n",
      "Size test test: 936\n",
      "number topics: 2\n",
      "perplexity: -171080.062304\n",
      "perpexity alternative measure_1: -6.40650853845\n",
      "perpexity alternative measure_2: 84.8305824643\n",
      "number of words: 26704.0\n",
      "per-word perplexity: 84.8317953576\n",
      "number topics: 12\n",
      "perplexity: -207908.555901\n",
      "perpexity alternative measure_1: -7.78386408669\n",
      "perpexity alternative measure_2: 220.531218418\n",
      "number of words: 26704.0\n",
      "per-word perplexity: 220.658407424\n",
      "number topics: 22\n",
      "perplexity: -238761.513564\n",
      "perpexity alternative measure_1: -8.94305645986\n",
      "perpexity alternative measure_2: 491.098616998\n",
      "number of words: 26704.0\n",
      "per-word perplexity: 491.497171427\n",
      "number topics: 32\n",
      "perplexity: -267594.974683\n",
      "perpexity alternative measure_1: -10.0202991747\n",
      "perpexity alternative measure_2: 1039.81854348\n",
      "number of words: 26704.0\n",
      "per-word perplexity: 1038.85778591\n",
      "number topics: 42\n",
      "perplexity: -293453.802589\n",
      "perpexity alternative measure_1: -10.9900637455\n",
      "perpexity alternative measure_2: 2038.2817342\n",
      "number of words: 26704.0\n",
      "per-word perplexity: 2032.63127004\n"
     ]
    }
   ],
   "source": [
    "# Set the different numbers of topics that I will try\n",
    "num_topic_list = range(2, 52, 10)\n",
    "\n",
    "# Set number of cores to use when running gensim\n",
    "my_workers=8\n",
    "\n",
    "# Read in dictionary and corpus\n",
    "my_dictionary = corpora.Dictionary.load('../data/lda_dictionary.dict')\n",
    "my_corpus = corpora.MmCorpus('../data/lda_corpus.mm')\n",
    "\n",
    "# Split into 80% training and 20% test sets\n",
    "cp = list(my_corpus)\n",
    "random.shuffle(cp)\n",
    "p = int(len(cp) * .8)\n",
    "train_corpus = cp[0:p]\n",
    "test_corpus = cp[p:]\n",
    "print 'Size training set:', len(train_corpus)\n",
    "print 'Size test test:', len(test_corpus) \n",
    "\n",
    "# Compute perplexities for different numbers of topics\n",
    "perplexities = []\n",
    "my_chunksize = int(float(len(my_corpus))/my_workers)\n",
    "number_of_words = sum(cnt for document in test_corpus for _, cnt in document)\n",
    "for n in num_topic_list:\n",
    "    model = gensim.models.LdaMulticore(corpus=train_corpus, num_topics=n, id2word=my_dictionary, \n",
    "                                       workers=my_workers, chunksize=my_chunksize, iterations=10)\n",
    "\n",
    "    # calculate per-word perplexity\n",
    "    perplex = model.bound(test_corpus) # this is model perplexity not the per word perplexity\n",
    "    per_word_perplex = np.exp2(-perplex / number_of_words)\n",
    "    perplex_1 = model.log_perplexity(test_corpus, total_docs=len(test_corpus))\n",
    "    perplex_2 = 2.0**(-model.log_perplexity(test_corpus, total_docs=len(test_corpus)))\n",
    "    # per_word_perplex = np.exp2(-lda.log_perplexity(cp_test))\n",
    "    # Alternative way of getting per-word perplexity\n",
    "    # per_word_perplex = np.exp2(-lda.bound(cp_test) / sum(cnt for document in cp_test for _, cnt in document))\n",
    "\n",
    "    perplexities.append(per_word_perplex)\n",
    "    print 'number topics:', n\n",
    "    print 'number of words:', number_of_words\n",
    "    print 'perplexity:', perplex\n",
    "    print 'per-word perplexity:', per_word_perplex\n",
    "    print 'perpexity alternative measure_1:', perplex_1\n",
    "    print 'perpexity alternative measure_2:', perplex_2\n",
    "    \n",
    "    \n",
    "   \n",
    "topics_perps_lists = [num_topic_list, perplexities]\n",
    "\n",
    "# Write lists of number of topics and corresponding perplexities to json\n",
    "with open('../results/topics_perps_lists.txt', 'w') as myfile:\n",
    "    json.dump(topics_perps_lists, myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009765625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 12, 22, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "print num_topic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then read in the results, and plot perplexity against the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqXiIiEijqXiIiEij/X/l3WXo/YoyVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5b034d96d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('../results/topics_perps_lists.txt','r') as infile:\n",
    "    newList = json.load(infile)\n",
    "    \n",
    "plt.plot(newList[0], newList[1],'-o', markersize=5)\n",
    "#plt.ylim([0,2000])\n",
    "plt.ylabel('per-word perplexity')\n",
    "plt.xlabel('number topics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that per-word perplexity is increasing in the number of topics. After some digging around on Google, it appears that there is an issue here with gensim. Lots of people have had the same problem (for example, [article 1](https://groups.google.com/forum/#!topic/gensim/iK692kdShi4), [article2](https://groups.google.com/forum/#!topic/gensim/TpuYRxhyIOc), [article 3](http://stackoverflow.com/questions/36913218/lda-with-gensim-strange-values-for-perplexity)) and it does not appear to have been resolved. \n",
    "\n",
    "In light of this issue with gensim, I ignore the elbow method. I instead spent some time experimeting with different numbers of topics to find a value of k that grouped the blurbs into meaninful groups and was not so large it would make interpretation challenging. I settled on eight topics.\n",
    "\n",
    "## Choose hyperparameter values (alpha and eta)\n",
    "\n",
    "I had originally indended to conduct a grid search to explore how perplexity varies with different values of alpha and eta. [However, given the problems with calcuating perplexity when using gensim, I instead decided to use the deafult values of alpha and eta.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set number topics\n",
    "k = 10\n",
    "\n",
    "## Set the parameter values that I will search over\n",
    "param_list = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5]\n",
    "\n",
    "# Read in dictionary and corpus\n",
    "my_dictionary = corpora.Dictionary.load('../Data/lda_dictionary.dict')\n",
    "my_corpus = corpora.MmCorpus('../Data/lda_corpus.mm')\n",
    "\n",
    "# Split into 80% training and 20% test sets\n",
    "cp = list(my_corpus)\n",
    "random.shuffle(cp)\n",
    "p = int(len(cp) * .8)\n",
    "cp_train = cp[0:p]\n",
    "cp_test = cp[p:]\n",
    "\n",
    "# Compute perplexities\n",
    "grid_vals = {}\n",
    "for alpha_val in param_list:\n",
    "    perp_vals = []\n",
    "    for eta_val in param_list:\n",
    "        lda = gensim.models.ldamodel.LdaModel(cp_train, num_topics=k, alpha=alpha_val, eta=eta_val, id2word=my_dictionary, passes=20)\n",
    "        per_word_perplex = np.exp2(-lda.log_perplexity(cp_test))\n",
    "        perp_vals.append(per_word_perplex)\n",
    "    grid_vals[alpha_val] = perp_vals\n",
    "\n",
    "# Convert dictionary to DataFrame, with alpha on x axis, and eta on y axis\n",
    "final_grid = pd.DataFrame(grid_vals, index=param_list)\n",
    "\n",
    "# Write grid of perplexities to csv\n",
    "final_grid.to_csv('../Results/alpha_eta_grid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model with chosen parameters\n",
    "\n",
    "I now run my chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mport logging\n",
    "from gensim import corpora\n",
    "\n",
    "my_topics = 8\n",
    "my_workers = 8\n",
    " \n",
    "# To see output as code is running, uncomment line below\n",
    "# logging.basicConfig(format='%(asctime)s: %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "data_directory = '../Data/'\n",
    "my_dict = corpora.Dictionary.load(data_directory + 'lda_dictionary.dict')\n",
    "my_corpus = corpora.MmCorpus(data_directory + 'lda_corpus.mm')\n",
    "\n",
    "my_chunksize = int(float(len(my_corpus))/my_workers)\n",
    "my_lda = gensim.models.LdaMulticore(my_corpus, num_topics=my_topics, id2word=my_dict, passes=30, workers=my_workers, chunksize=my_chunksize)\n",
    "my_lda.save('../Results/lda_8topics.lda')\n",
    "\n",
    "topic_list  = my_lda.print_topics(num_topics=my_topics, num_words=8)\n",
    "for t in topic_list:\n",
    "    print t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then looked at the top words for each topic (to give each topic a name) and topic proportions for each blurb (which I will use in my analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_lda = gensim.models.LdaModel.load('../aws_dir/lda_project/Results/lda_8topics.lda')\n",
    "\n",
    "# Highest probability words in each topic\n",
    "topic_list  = fin_lda.print_topics(num_topics=10, num_words=10)\n",
    "for t in topic_list:\n",
    "    s = t[1]\n",
    "    s = ''.join([c for c in s if not (c.isdigit() or c==\"*\" or c==\".\" or c=='\"')])\n",
    "    print t[0], s\n",
    "    print\n",
    "\n",
    "# Topics in each blurb\n",
    "for row in range(3): \n",
    "    print test_lda[corpus[row]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the topics for each blurb into a dataframe where each blurb is a row, and each topic colum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicmatrix = []\n",
    "for i in range(len(corpus)):\n",
    "    old_row = test_lda[corpus[i]]\n",
    "    new_row = [0]*8\n",
    "    for j in old_row:\n",
    "        new_row[j[0]] = j[1]\n",
    "    topicmatrix.append(new_row)\n",
    "\n",
    "df = pd.DataFrame(topicmatrix)\n",
    "print len(df)\n",
    "\n",
    "## Drop columns where topic values zero\n",
    "print sum(df.ix[:,6])\n",
    "print sum(df.ix[:,7])\n",
    "df = df.ix[:,0:5]\n",
    "\n",
    "# Rename columns and add URN\n",
    "df.columns = ['Offerings', 'Happiness', 'Inspections', 'Success', 'Support', 'Development']\n",
    "df['URN'] = urn_series\n",
    "\n",
    "# Reorder columns and reset index\n",
    "df = df[['URN', 'Offerings', 'Happiness', 'Inspections', 'Success', 'Support', 'Development']]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save\n",
    "df.to_csv('../my_datasets/lda_dat.csv')\n",
    "\n",
    "# Check df looks ok (put at end so get nice table in Jupyter)\n",
    "print len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving my LDA analysis\n",
    "\n",
    "There are at least two ways in which I should like to improve my LDA analysis. First, I should like to address the issue of calculating perplexity, and find optimal values for k, alpha, and eta. Probably the best approach would be to find a different measure for evaluating the models. Perplexity would not be ideal even if it worked, as it is a measure of predictive performance, but I am not using LDA for the purposes of prediction.\n",
    "\n",
    "Second, I should like to explore different algorithms for approximating the posterior/fitting my model. So far in training my LDA model I have only used [\"online variational Bayes\"](https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf), the [method built into gensim](https://rare-technologies.com/multicore-lda-in-python-from-over-night-to-over-lunch/). I should like to instead try repeating my analsis with other methods, such as Gibbs sampling.  There is a wrapper in Python that should allow me to use Gibbs sampling with Gensim; however, it might instead be easier to use the LDA method in graphlab. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
