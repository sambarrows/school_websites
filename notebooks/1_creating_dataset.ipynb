{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Text from School Websites\n",
    "\n",
    "I investigate what types of information schools in England choose to present on their websites. \n",
    "\n",
    "\"Information unravelling\" is one of the most well-known theories that explains the information that organizations voluntarily disclose. This theory posits that organizations disclose information about their quality if this information is favorable. Therefore, consumers can infer that organizations that do not disclose information are likely to have worse quality than firms that do. Through an unravelling process, higher-quality firms have an incentive to reveal information about their quality, which then puts pressure on all firms to disclose. \n",
    "\n",
    "If information unravelling holds true in the case of schools, then parents can broadly trust that school websites will disclose relevant information and may want to use these websites in their school search.\n",
    "\n",
    "However, recent work has shown that patterns of disclosure can be different in real-world settings, such when organizations have multiple different types of information that they can disclose. [Luca & Smith (2015)](http://www.sciencedirect.com/science/article/pii/S0167268114003369), for example, find that top business schools are actually less likely to disclose their rankings than mid-ranked business schools. Under this \"countersignalling,\" not disclosing this information signals that schools are confident that their ranking will be favorable (Luca & Smith also have a nice discussion theories of information disclosure, which I have drawn on here).\n",
    "\n",
    "If schools are selectively disclosing information, it is important that parents understand this. If a school fails to mention it's test-score performance or school inpsection results, for example, is this a sign that you should be on the look out for problems? Or, in fact, do the best schools not bother to mention their performance as it can be assumed?\n",
    "\n",
    "By exploring these questions here, I hope to help parents better understand how they should approach the information that schools provide on their websites.\n",
    "\n",
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import gensim\n",
    "import json\n",
    "import random\n",
    "import webbrowser\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize                  \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora, models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "\n",
    "I investigate how schools present themselves on their websites by analyzing text from the homepages of school websites. Specifically, I focus on the introductory \"blurb\" found on many school websites, for example:\n",
    "\n",
    "<img src=\"blurb_example.png\">\n",
    "\n",
    "I began by downloading a list of 26,641 urls for the websites of schools in England from the Department of Education's [Edubase public portal](http://www.education.gov.uk/edubase/home.xhtml). I then scraped the text from all of the working URLs using [Scrapy](https://scrapy.org/). ***SHOULD EXPLAIN THAT I DID THIS ON EC2, AND NOTE NAME OF DIRECTORY***\n",
    "\n",
    "The primary difficulty in building my scraper was that schools have text about all kinds of things on their homepage, and comparing, for example, the introductory blurb from one website with a news story from another is unlikely to yield much of interest. Further complicating matters, the construction of websites is inconsistent across schools, making it difficult to easily capture the introductory blurb.\n",
    "\n",
    "I began by taking the largest block of text that I could find on each page, but this missed a lot of the blurbs I wanted to catch, only caught part of some blurbs, and picked up a lot of other mess. However, I noticed that many blurbs either started with \"Welcome\", or ended with the headteacher signing off.\n",
    "\n",
    "I therefore wrote a spider that, in short, would collect paragraphs of text below \"Welcome\" or above \"Headteacher\", \"Head of School\", or various other words used for principal. I limited the blurbs I scraped to strings of 200 words or greater.\n",
    "\n",
    "## Initial Exploration\n",
    "\n",
    "I take a look at the number of Establishments in the EduBase dataset for which I have urls, the url was working, I was able to scrape some text etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establishments in EduBase dataset: 45143\n",
      "Obs before scraping (obs for which have URLs): 26641\n",
      "Obs after scraping: 26641\n",
      "\n",
      "Looking at observations after scraping....\n",
      "\n",
      "URL worked but no blurb: 5388\n",
      "\n",
      "Total errors: 2825\n",
      "Other error: 675\n",
      "HTTP error: 534\n",
      "DNS error: 1597\n",
      "Timeout error: 19\n",
      "\n",
      "Total found blurb: 18428\n",
      "Blurbs >4k characters: 37\n",
      "Blurbs <250 characters: 12861\n",
      "Blurbs >=250 & <=4k characters: 5530\n",
      "\n",
      "Check totals sum correctly: 26641\n"
     ]
    }
   ],
   "source": [
    "edubase_dat = pd.read_csv('../datasets/edubase_datasets/edubasealldata.csv', low_memory=False)\n",
    "url_dat = pd.read_csv('../Scraping/csvurls.csv', names=['URN','URL'])  ##urls after dropping nulls\n",
    "blurbs_dat = pd.read_csv('../Scraping/blurbs_spider/blurbs_dat.csv')\n",
    "\n",
    "print 'Establishments in EduBase dataset:', len(edubase_dat)\n",
    "print 'Obs before scraping (obs for which have URLs):', len(url_dat)\n",
    "print 'Obs after scraping:', len(blurbs_dat)\n",
    "print\n",
    "print 'Looking at observations after scraping....'\n",
    "print \n",
    "print 'URL worked but no blurb:', sum(blurbs_dat['flag']=='Found_url_but_no_blurb')\n",
    "print\n",
    "total_errors = sum(blurbs_dat['flag']=='Other_error') + sum(blurbs_dat['flag']=='HTTP_error') + \\\n",
    "        sum(blurbs_dat['flag']=='DNS_error') + sum(blurbs_dat['flag']=='Timeout_error')\n",
    "print 'Total errors:', total_errors\n",
    "print 'Other error:', sum(blurbs_dat['flag']=='Other_error')\n",
    "print 'HTTP error:', sum(blurbs_dat['flag']=='HTTP_error')\n",
    "print 'DNS error:', sum(blurbs_dat['flag']=='DNS_error')\n",
    "print 'Timeout error:', sum(blurbs_dat['flag']=='Timeout_error')\n",
    "print\n",
    "\n",
    "found_blurb = blurbs_dat[blurbs_dat['flag']=='Found_blurb']\n",
    "print 'Total found blurb:', len(found_blurb) \n",
    "\n",
    "## Take a look at longest blurbs\n",
    "# print sorted(list(found_blurb['length']), reverse=True)[:100]\n",
    "# for x in sorted(list(found_blurb['length']), reverse=True)[:20]:\n",
    "#     print x\n",
    "#     print list(found_blurb.loc[found_blurb['length']==x, 'blurb'])  \n",
    "\n",
    "## Take a look at shortest blurbs\n",
    "# print sorted(list(found_blurb['length']))[:100]\n",
    "# for x in sorted(list(found_blurb['length']))[:20]:\n",
    "#     print x\n",
    "#     print list(found_blurb.loc[found_blurb['length']==x, 'blurb']) \n",
    "\n",
    "## Take a look at blurbs between 240 and 250 characters\n",
    "# print list(found_blurb.loc[(found_blurb['length']<250) & (found_blurb['length']>240), 'blurb'])\n",
    "\n",
    "print 'Blurbs >4k characters:', len(found_blurb[found_blurb['length']>4000])  \n",
    "print 'Blurbs <250 characters:', len(found_blurb[found_blurb['length']<250])  \n",
    "print 'Blurbs >=250 & <=4k characters:', len(found_blurb[(found_blurb['length']>=250) & (found_blurb['length']<=4000)])\n",
    "print\n",
    "\n",
    "print 'Check totals sum correctly:', sum(blurbs_dat['flag']=='Found_url_but_no_blurb') + total_errors + len(found_blurb) \n",
    "\n",
    "## Delete unneeded datsets to free up space\n",
    "del found_blurb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the blurbs of more than 4000 characters in length appear to have picked up text I wasn't aiming for, such as separate blocks of text on school news. Under 4000, in contrast, the blurbs I examined appear to be just legitimately long blurbs. Below I will therefore drop the 15 blurbs that are longer than 4000 characters.\n",
    "    \n",
    "Likewise, starting with the shortest blurbs, it is only when we reach 200-250 characters that the text starts to represent a meaningful welcome blurb with any information about the school. Below this, I either captured only part of the blurb (for example, in many case I just picked up \"Welcome\") or, in most cases, the blurb just said welcome to the webiste and gave little additional information. I will therefore limit the sample to blurbs of 250 or more characters.\n",
    "\n",
    "## Dropping observations\n",
    "\n",
    "### Restrict to open state-funded and independent schools\n",
    "\n",
    "I first restrict my sample to all open state-funded and independent schools. I exclude both closed establishments that are included in the original database, as well as other types of establishment such as high education institutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EduBase dataset contains no duplicate URNs: True\n",
      "Database after scraping contains no duplicate URNs: True\n",
      "State datset contains no dublicate URNs: True\n",
      "\n",
      "Obs in state funded schools dataset: 21917\n",
      "Obs after limiting edubase to establishments in state funded schools dataset: 21917\n",
      "Reduced Edubase dataset and state dataset use same school types: True\n",
      "State schools obs: 21917\n",
      "Independent schools obs: 2306\n",
      "Obs in state and indep dataset equals totals: True\n",
      "Obs in state and indep dataset: 24223\n",
      "\n",
      "Obs in blurbs dataset  26641\n",
      "Obs after restricting blurbs dataset to open state and independent schools: 24223\n",
      "\n",
      "Re-run summary statistics on dataset of open state and independent schools:\n",
      "Schools in EduBase dataset: 24223\n",
      "Obs for which have URLs: 20771\n",
      "Obs after scraping: 20771\n",
      "\n",
      "Looking at obs after scraping:\n",
      "URL worked but no blurb: 4160\n",
      "\n",
      "Total errors: 814\n",
      "Other error: 173\n",
      "HTTP error: 238\n",
      "DNS error: 395\n",
      "Timeout error: 8\n",
      "\n",
      "Total found blurb: 15797\n",
      "Blurbs >4k characters: 29\n",
      "Blurbs <250 characters: 11018\n",
      "Blurbs >=250 & <=4k characters: 4750\n"
     ]
    }
   ],
   "source": [
    "## Take a look at column names in edubase\n",
    "# print 'Column names:'\n",
    "# for col_name in list(edubase_dat.columns.values):\n",
    "#     print col_name\n",
    "# print \n",
    "\n",
    "## Select key variables from edubase\n",
    "edubase = edubase_dat.copy()\n",
    "edubase = edubase[['URN', 'SchoolWebsite', 'TypeOfEstablishment (name)', 'EstablishmentStatus (name)']]\n",
    "\n",
    "## Rename columns\n",
    "edubase.columns = ['TypeOfEstablishment' if x=='TypeOfEstablishment (name)' else x for x in edubase.columns]\n",
    "edubase.columns = ['EstablishmentStatus' if x=='EstablishmentStatus (name)' else x for x in edubase.columns]\n",
    "\n",
    "## Check that URN is a unique identifier in edubase and blurbs datasets\n",
    "print 'EduBase dataset contains no duplicate URNs:', len(edubase['URN'].unique())== len(edubase)\n",
    "print 'Database after scraping contains no duplicate URNs:', len(blurbs_dat['urn'].unique()) == len(blurbs_dat)\n",
    "\n",
    "## Load dataset of open state funded schools and get list of URNS\n",
    "## (I didn't use this database for scraping because I wanted to get indepedent schools)\n",
    "state = pd.read_csv('../edubase_datasets/edubaseallstatefunded20170218.csv', low_memory=False)\n",
    "state = state[['URN', 'TypeOfEstablishment (name)', 'EstablishmentStatus (name)']]\n",
    "state.columns = ['URN', 'State_df_type', 'State_df_status']\n",
    "print 'State datset contains no dublicate URNs:', len(state['URN'].unique())== len(state)\n",
    "print\n",
    "print 'Obs in state funded schools dataset:', len(state)\n",
    "\n",
    "## Select obs from edubase where URN is in open state funded schools dataset\n",
    "ed_state = pd.merge(state, edubase, how='left', on='URN')\n",
    "print 'Obs after limiting edubase to establishments in state funded schools dataset:', len(ed_state)\n",
    "\n",
    "## Confirm that reduced edubase and state school datasets use same coding for school type\n",
    "print 'Reduced Edubase dataset and state dataset use same school types:', \\\n",
    "len(sorted(list(ed_state['TypeOfEstablishment'].unique()))) == \\\n",
    "len(sorted(list(state['State_df_type'].unique())))\n",
    "del ed_state['State_df_type']\n",
    "del ed_state['State_df_status']\n",
    "\n",
    "## Take a look at the school types in my dataset of open state funded schools\n",
    "# print 'Number of school types in state dataset:', len(sorted(list(ed_state['TypeOfEstablishment'].unique())))\n",
    "# print\n",
    "# for school_type in sorted(list(ed_state['TypeOfEstablishment'].unique())):\n",
    "#     print school_type\n",
    "# print\n",
    "\n",
    "## Select obs from edubase where URN in open state funded schools database\n",
    "## and take a look at school types included\n",
    "ed_other = edubase[~edubase.TypeOfEstablishment.isin(list(ed_state['TypeOfEstablishment'].unique()))]\n",
    "# for school_type in sorted(list(ed_other['TypeOfEstablishment'].unique())):\n",
    "#     print school_type\n",
    "# print\n",
    "    \n",
    "## Select independent schools from edubase dataset\n",
    "ed_ind = edubase[((edubase['TypeOfEstablishment']=='Other Independent School') |\\\n",
    "(edubase['TypeOfEstablishment']=='Other Independent Special School')) &\\\n",
    "((edubase['EstablishmentStatus']=='Open')|\\\n",
    "(edubase['EstablishmentStatus']=='Open, but proposed to close'))]\n",
    "\n",
    "## Bind dataset of open state schools with open independent schools\n",
    "## (nb. still just working with Edubase data - no blurbs)\n",
    "ed_schools = pd.concat([ed_state, ed_ind], ignore_index=True)\n",
    "print 'State schools obs:', len(ed_state)\n",
    "print 'Independent schools obs:', len(ed_ind)\n",
    "print 'Obs in state and indep dataset equals totals:', len(ed_schools) == len(ed_state) + len(ed_ind)\n",
    "print 'Obs in state and indep dataset:', len(ed_schools)\n",
    "print \n",
    "\n",
    "## Restrict blurbs dataset to state and independent schools\n",
    "## (by merging with reduced edubase dataset I just made)\n",
    "blurbs_dat['scraped_dataset'] = True\n",
    "print 'Obs in blurbs dataset ', len(blurbs_dat)\n",
    "df = pd.merge(ed_schools, blurbs_dat, how='left', left_on='URN', right_on='urn')\n",
    "print 'Obs after restricting blurbs dataset to open state and independent schools:', len(df)\n",
    "print\n",
    "\n",
    "print \"Re-run summary statistics on dataset of open state and independent schools:\"\n",
    "def scrapy_stats(ed_schools, df): \n",
    "    print 'Schools in EduBase dataset:', len(ed_schools)\n",
    "    print 'Obs for which have URLs:', sum(ed_schools.SchoolWebsite.notnull())\n",
    "    print 'Obs after scraping:', sum(df['scraped_dataset']==True)\n",
    "    print\n",
    "    print 'Looking at obs after scraping:'\n",
    "    print 'URL worked but no blurb:', sum(df['flag']=='Found_url_but_no_blurb')\n",
    "    print \n",
    "    total_errors = sum(df['flag']=='Other_error') + sum(df['flag']=='HTTP_error') + \\\n",
    "            sum(df['flag']=='DNS_error') + sum(df['flag']=='Timeout_error')\n",
    "    print 'Total errors:', total_errors\n",
    "    print 'Other error:', sum(df['flag']=='Other_error')\n",
    "    print 'HTTP error:', sum(df['flag']=='HTTP_error')\n",
    "    print 'DNS error:', sum(df['flag']=='DNS_error')\n",
    "    print 'Timeout error:', sum(df['flag']=='Timeout_error')\n",
    "    print \n",
    "    df_found_blurb = df[df['flag']=='Found_blurb']\n",
    "    print 'Total found blurb:', len(df_found_blurb) \n",
    "    print 'Blurbs >4k characters:', len(df_found_blurb[df_found_blurb['length']>4000])  \n",
    "    print 'Blurbs <250 characters:', len(df_found_blurb[df_found_blurb['length']<250])  \n",
    "    print 'Blurbs >=250 & <=4k characters:', len(df_found_blurb[(df_found_blurb['length']>=250) & \\\n",
    "                                                                (df_found_blurb['length']<=4000)])\n",
    "scrapy_stats(ed_schools, df)\n",
    "\n",
    "# # Create histogram of observations I will not be dropping\n",
    "# # (old code - but could be worth revising)\n",
    "# dat_red = dat[dat['length']<=4000]\n",
    "# dat_red = dat_red[dat_red['length']>=250]\n",
    "\n",
    "# plt.hist(list(dat_red['length']), bins=100)\n",
    "# plt.show()\n",
    "\n",
    "# dat_small = dat[dat['length']<=250]\n",
    "# plt.hist(list(dat_small['length']), bins=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop additional school types\n",
    "\n",
    "Many different types of schools fall within my broader grouping of open state funded schools. I want to explore how the proportion of blurbs I have collected varies across different more specific school types, as it may affect which I keep in my sample. However, the database currently includes 24 different school types. I first, therefore, create a new variable that groups school types, and then compare the proportion of blurbs collected for these collapsed types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sch_type</th>\n",
       "      <th>edubase_tot</th>\n",
       "      <th>url_tot</th>\n",
       "      <th>scrape_tot</th>\n",
       "      <th>url_perc</th>\n",
       "      <th>blurb_tot</th>\n",
       "      <th>blurb_perc</th>\n",
       "      <th>blurb_perc_of_urls</th>\n",
       "      <th>med_tot</th>\n",
       "      <th>med_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Academy</td>\n",
       "      <td>6033</td>\n",
       "      <td>5480</td>\n",
       "      <td>5480</td>\n",
       "      <td>0.908337</td>\n",
       "      <td>4246</td>\n",
       "      <td>0.703796</td>\n",
       "      <td>0.774818</td>\n",
       "      <td>1461</td>\n",
       "      <td>0.242168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Community</td>\n",
       "      <td>8438</td>\n",
       "      <td>7863</td>\n",
       "      <td>7863</td>\n",
       "      <td>0.931856</td>\n",
       "      <td>6030</td>\n",
       "      <td>0.714624</td>\n",
       "      <td>0.766883</td>\n",
       "      <td>1676</td>\n",
       "      <td>0.198625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>973</td>\n",
       "      <td>919</td>\n",
       "      <td>919</td>\n",
       "      <td>0.944502</td>\n",
       "      <td>676</td>\n",
       "      <td>0.694758</td>\n",
       "      <td>0.735582</td>\n",
       "      <td>194</td>\n",
       "      <td>0.199383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Free</td>\n",
       "      <td>345</td>\n",
       "      <td>245</td>\n",
       "      <td>245</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>192</td>\n",
       "      <td>0.556522</td>\n",
       "      <td>0.783673</td>\n",
       "      <td>60</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Independent</td>\n",
       "      <td>2306</td>\n",
       "      <td>839</td>\n",
       "      <td>839</td>\n",
       "      <td>0.363833</td>\n",
       "      <td>538</td>\n",
       "      <td>0.233304</td>\n",
       "      <td>0.641240</td>\n",
       "      <td>173</td>\n",
       "      <td>0.075022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LA Nursery School</td>\n",
       "      <td>402</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>0.490050</td>\n",
       "      <td>129</td>\n",
       "      <td>0.320896</td>\n",
       "      <td>0.654822</td>\n",
       "      <td>38</td>\n",
       "      <td>0.094527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pupil Referral Unit</td>\n",
       "      <td>258</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>0.375969</td>\n",
       "      <td>66</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>19</td>\n",
       "      <td>0.073643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Studio Schools</td>\n",
       "      <td>36</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>20</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>8</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>University Technical College</td>\n",
       "      <td>48</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>29</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>10</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Voluntary</td>\n",
       "      <td>5384</td>\n",
       "      <td>5069</td>\n",
       "      <td>5069</td>\n",
       "      <td>0.941493</td>\n",
       "      <td>3871</td>\n",
       "      <td>0.718982</td>\n",
       "      <td>0.763661</td>\n",
       "      <td>1111</td>\n",
       "      <td>0.206352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sch_type  edubase_tot  url_tot  scrape_tot  url_perc  \\\n",
       "0                       Academy         6033     5480        5480  0.908337   \n",
       "1                     Community         8438     7863        7863  0.931856   \n",
       "2                    Foundation          973      919         919  0.944502   \n",
       "3                          Free          345      245         245  0.710145   \n",
       "4                   Independent         2306      839         839  0.363833   \n",
       "5             LA Nursery School          402      197         197  0.490050   \n",
       "6           Pupil Referral Unit          258       97          97  0.375969   \n",
       "7                Studio Schools           36       26          26  0.722222   \n",
       "8  University Technical College           48       36          36  0.750000   \n",
       "9                     Voluntary         5384     5069        5069  0.941493   \n",
       "\n",
       "   blurb_tot  blurb_perc  blurb_perc_of_urls  med_tot  med_perc  \n",
       "0       4246    0.703796            0.774818     1461  0.242168  \n",
       "1       6030    0.714624            0.766883     1676  0.198625  \n",
       "2        676    0.694758            0.735582      194  0.199383  \n",
       "3        192    0.556522            0.783673       60  0.173913  \n",
       "4        538    0.233304            0.641240      173  0.075022  \n",
       "5        129    0.320896            0.654822       38  0.094527  \n",
       "6         66    0.255814            0.680412       19  0.073643  \n",
       "7         20    0.555556            0.769231        8  0.222222  \n",
       "8         29    0.604167            0.805556       10  0.208333  \n",
       "9       3871    0.718982            0.763661     1111  0.206352  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for s in sorted(list(df['TypeOfEstablishment'].unique())):\n",
    "#     print s\n",
    "# print\n",
    "\n",
    "def collapse_types(df):\n",
    "    df['school_type'] = df['TypeOfEstablishment']\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:7]=='Academy', 'school_type'] ='Academy'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:9]=='Community', 'school_type'] ='Community'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:10]=='Foundation', 'school_type'] ='Foundation'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:4]=='Free', 'school_type'] ='Free'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:5]=='Other', 'school_type'] ='Independent'\n",
    "    df.ix[df.TypeOfEstablishment.astype(str).str[:9]=='Voluntary', 'school_type'] ='Voluntary'\n",
    "    return df\n",
    "df = collapse_types(df)\n",
    "\n",
    "## Create table\n",
    "sch_type = []\n",
    "edubase_tot = []\n",
    "url_tot = []\n",
    "scrape_tot = []\n",
    "url_perc = []\n",
    "blurb_tot = []\n",
    "blurb_perc = []\n",
    "blurb_perc_of_urls = []\n",
    "med_tot = []\n",
    "med_perc = []\n",
    "for s in sorted(list(df['school_type'].unique())):\n",
    "    sub = df[df.school_type==s]\n",
    "    sch_type.append(s)\n",
    "    edubase_tot.append(len(sub))\n",
    "    url_tot.append(sum(sub.SchoolWebsite.notnull()))\n",
    "    scrape_tot.append(sum(sub['scraped_dataset']==True))\n",
    "    url_perc.append(float(sum(sub['scraped_dataset']==True))/float(len(sub)))\n",
    "    blurb_tot.append(sum(sub['flag']=='Found_blurb'))\n",
    "    blurb_perc.append(float(sum(sub['flag']=='Found_blurb'))/float(len(sub)))\n",
    "    blurb_perc_of_urls.append(float(sum(sub['flag']=='Found_blurb'))/float(sum(sub['scraped_dataset']==True)))\n",
    "    sub_blurb = sub[sub['flag']=='Found_blurb']\n",
    "    med_tot.append(len(sub_blurb[(sub_blurb['length']>=250) & (sub_blurb['length']<=4000)]))\n",
    "    med_perc.append(float(len(sub_blurb[(sub_blurb['length']>=250) & (sub_blurb['length']<=4000)]))/float(len(sub)))\n",
    "df_sch_type = pd.DataFrame({'sch_type': sch_type})\n",
    "df_sch_type['edubase_tot'] = edubase_tot\n",
    "df_sch_type['url_tot'] = url_tot\n",
    "df_sch_type['scrape_tot'] = scrape_tot\n",
    "df_sch_type['url_perc'] = url_perc\n",
    "df_sch_type['blurb_tot'] = blurb_tot\n",
    "df_sch_type['blurb_perc'] = blurb_perc\n",
    "df_sch_type['blurb_perc_of_urls'] = blurb_perc_of_urls\n",
    "df_sch_type['med_tot'] = med_tot\n",
    "df_sch_type['med_perc'] = med_perc\n",
    "df_sch_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of this table, I will drop pupil referall units, studio schools, and university technical colleges because there are so few of them. I also drop nursery schools, as they are a different type of entity. I then take another look at the summary statistics for this reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df and reduced edubase dataset have same # school types: True\n",
      "\n",
      "Final school types:\n",
      "Academy\n",
      "Community\n",
      "Foundation\n",
      "Free\n",
      "Independent\n",
      "Voluntary\n",
      "\n",
      "Schools in EduBase dataset: 23479\n",
      "Obs for which have URLs: 20415\n",
      "Obs after scraping: 20415\n",
      "\n",
      "Looking at obs after scraping:\n",
      "URL worked but no blurb: 4071\n",
      "\n",
      "Total errors: 791\n",
      "Other error: 167\n",
      "HTTP error: 235\n",
      "DNS error: 381\n",
      "Timeout error: 8\n",
      "\n",
      "Total found blurb: 15553\n",
      "Blurbs >4k characters: 29\n",
      "Blurbs <250 characters: 10849\n",
      "Blurbs >=250 & <=4k characters: 4675\n"
     ]
    }
   ],
   "source": [
    "df = df[(df['school_type']!='LA Nursery School')&\\\n",
    "        (df['school_type']!='Pupil Referral Unit')&\\\n",
    "        (df['school_type']!='Studio Schools')&\\\n",
    "        (df['school_type']!='University Technical College')]\n",
    "\n",
    "## Drop same collapsed school types from ed_schools (needed to get summary stats)\n",
    "ed_schools = collapse_types(ed_schools)\n",
    "ed_schools = ed_schools[(ed_schools['school_type']!='LA Nursery School')&\\\n",
    "                        (ed_schools['school_type']!='Pupil Referral Unit')&\\\n",
    "                        (ed_schools['school_type']!='Studio Schools')&\\\n",
    "                        (ed_schools['school_type']!='University Technical College')]\n",
    "\n",
    "## Get list of additional school types to drop from ed_schools\n",
    "sch_drops = []\n",
    "for school_type in sorted(list(ed_other['TypeOfEstablishment'].unique())):\n",
    "    sch_drops.append(school_type)\n",
    "sch_drops = [x for x in sch_drops if x != 'Other Independent School']\n",
    "sch_drops = [x for x in sch_drops if x != 'Other Independent Special School']\n",
    "# print sch_drops\n",
    "\n",
    "## Drop those school types and check has same number of school types as df\n",
    "for school_type in sch_drops:\n",
    "    ed_schools = ed_schools[(ed_schools['TypeOfEstablishment']!=school_type)]\n",
    "print 'df and reduced edubase dataset have same # school types:', \\\n",
    "len(sorted(list(ed_schools['TypeOfEstablishment'].unique()))) == \\\n",
    "len(sorted(list(df['TypeOfEstablishment'].unique())))\n",
    "print\n",
    "\n",
    "## Take a look at remaining school types\n",
    "print 'Final school types:'\n",
    "for school_type in sorted(list(df['school_type'].unique())):\n",
    "    print school_type\n",
    "print\n",
    "\n",
    "## Get scraped sample summary stats\n",
    "scrapy_stats(ed_schools, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop observations by length of blurb\n",
    "\n",
    "I drop observations with too many or too few characters, and those with a null value in the blurb column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 23479\n",
      "Rows with NaN in blurb: 7926\n",
      "Non-null rows: 15553\n",
      "Non-short rows: 4708\n",
      "Final dataset rows: 4676\n"
     ]
    }
   ],
   "source": [
    "# Drop null values in blurb column\n",
    "print 'Total rows:', len(df)\n",
    "print 'Rows with NaN in blurb:', len(df[df.blurb.isnull()])\n",
    "df = df[df.blurb.notnull()]\n",
    "print 'Non-null rows:', len(df)\n",
    "\n",
    "# Drop rows where blurb shorter than 250 characters (or not there at all)\n",
    "df = df[df.blurb.str.len() > 249]\n",
    "print 'Non-short rows:', len(df)\n",
    "\n",
    "# Drop rows where blurb longer than 4000 characters\n",
    "df = df[df.blurb.str.len() < 4001]\n",
    "print 'Final dataset rows:', len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../my_datasets/prepped.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving my dataset\n",
    "\n",
    "I should like to obtain the URLs for more schools. I could do this my searching for schools and taking the top results using the Bing API. I should also like to try to improve my scraper to capture the blurbs from a higher proportion of school websites. To do improve my scraper, a first step would be to look through some of the websites for which I failed to scrape successfully. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
