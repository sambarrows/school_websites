{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Begin by separating URNs and blurbs, and dropping all non-ASCII characters from blurbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Get list of URNs (will need later when put it back together) and blurbs\n",
    "urn_list = df[\"urn\"].tolist()\n",
    "blist = df[\"blurb\"].tolist()\n",
    "# print df.head(50)\n",
    "\n",
    "##Remove all non-ASCII characters from blurbs (eg. \\xe2\\x80\\x98)\n",
    "new_list = []\n",
    "for b in blist:\n",
    "    new_list.append(b.decode('utf8').encode('ascii', errors='ignore'))\n",
    "blist_orig = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both my k-means and LDA analysis below I need to (in the following order):\n",
    "<ul>\n",
    "<li>**Tokenize**: divide string into a list of substrings </li>\n",
    "<li>**Remove stopwords**: stopwords are a list of high frequency words like, the, to, and also</li>\n",
    "<li>**Stem**: take the root of each word</li>\n",
    "</ul>\n",
    "\n",
    "In addition, I make all letters lower case and drop punctuation, and make sure each string/token contains letters and is longer than two characters.\n",
    "\n",
    "I will carry out all of these steps in the process of creating a TF-IDF matrix, below. But we need to prepare the functoins beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_punctuation_unicode(text):\n",
    "    '''.translate only takes str, whereas TfidfVectorizer only takes unicode.\n",
    "    Therefore, to use .translate in the tokenizer in TfidfVectorizer I need to\n",
    "    write a function that converts unicode -> string, applies .translate,\n",
    "    and then converts it back'''\n",
    "    str_text = str(text)\n",
    "    no_punctuation = str_text.translate(None, string.punctuation)\n",
    "    unicode_text = no_punctuation.decode('utf-8')\n",
    "    return unicode_text\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def prep_blurb(text):\n",
    "    lowers = text.lower()\n",
    "    no_punct = no_punctuation_unicode(lowers)\n",
    "    tokens = nltk.word_tokenize(no_punct)\n",
    "    has_letters = [t for t in tokens if re.search('[a-zA-Z]',t)]\n",
    "    no_numbers  = [t for t in has_letters if not hasNumbers(t)]\n",
    "    drop_stops = [w for w in no_numbers if not w in stoplist] \n",
    "    stems = [stemmer.stem(t) for t in drop_stops]\n",
    "    drop_short = [s for s in stems if len(s)>2]  ##not sure about this line\n",
    "    return drop_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF-IDF matrix\n",
    "\n",
    "To analyse this data, I need to convert it into numerical features. I next, therefore, extract a matrix of TF-IDF (term frequency-inverse document frequency) features.\n",
    "\n",
    "The tf-idf weight of a term in a document is the product of its tf (term frequency) weight and its idf (inverse document frequency) weight. This is generally given by the formula:\n",
    "\n",
    "$w_{t,d} = (1 + logtf_{t,d}) \\times log_{10}(\\frac{N}{df_t})$\n",
    "\n",
    "Where the term frequency $tf_{t,d}$ of term t in document d is defined as the number of times that t occurs in d, the document frequency $df_t$ is he number of documents that contain t, and N is the number of documents. The computation of tf-idfs in scikit-learn is in fact [slightly different from the standard textbook notation](http://scikit-learn.org/dev/modules/feature_extraction.html#the-bag-of-words-representation), but we can ignore that here.\n",
    "\n",
    "The tf-idf weight (of a term in a document) increases with the number of times a term occurs in a document, and also increases with the rarity of the term in the collection. Jurafsky and Manning have a [few videos](https://www.youtube.com/watch?v=PhunzHqhKoQ) giving a nice introduction to to tf-idf weighting and its components.\n",
    "\n",
    "Extracting tf-idf features gives us a weight matrix between terms and documents. Each document is represented by a row of the matrix, which is a real valued vector, and each term by a column. This will be a sparse matrix, that is, a matrix in which most of the elements are zero\n",
    "\n",
    "scikit-learn's TfidfVectorizer returns a matrix in scipy.sparse.csr format. Printing this matrix shows the nonzero values of the matrix in the (row, col) format.\n",
    "\n",
    "A few things to note about the parameters I define below (or previously experimented with defining):\n",
    "<ul>\n",
    "<li> max_df: the maximum frequency within the documents a given term can have to be used in the tfi-idf matrix.I stick with the default of 1.\n",
    "<li> min_idf: is the minimum frequecy a term can have to be included in the matrix. Can pass it either an integer (eg must occur 7 times) or a decimal (eg. must occur in at least .2 of the documents). I stick with the default of 1 (ie only needs to occur once).\n",
    "<li> ngram_range: the lower and upper boundary of the range of n-values for different n-grams to be extracted. An n-gram is basically a set of co-occuring words, and you typically move one for more word forward. For example, if n=2 (known as bigrams) then for the sentence \"The cow jumps over the moon\" the bigrams would be \"the cose\" \"cow jumps\" \"jumps over\" \"over the\" \"the moon\". I stick wiht the default of (1,1), that is, I only look at individual words.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61dc6020ef45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##Need to convert items in blist, as TfidfVectorizer only takes unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mblist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m##Create tf-idf matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blist' is not defined"
     ]
    }
   ],
   "source": [
    "## Convert items in blist, as TfidfVectorizer only takes unicode\n",
    "blist = [b.decode('utf-8') for b in blist]\n",
    "\n",
    "## Stopwords must also be in unicode to work with TfidfVectorizer\n",
    "stoplist = [word.decode('utf-8') for word in nltk.corpus.stopwords.words('english')] \n",
    "\n",
    "##Don't use stop_words argument to TfidfVectorizer as delt with in prep_blurb\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=prep_blurb)\n",
    "\n",
    "%time tf_idf = tfidf_vectorizer.fit_transform(blist)\n",
    "\n",
    "list_of_words = tfidf_vectorizer.get_feature_names()\n",
    "# print(tf_idf.shape)\n",
    "# print type(tf_idf)\n",
    "# print tfidf_matrix[:10,]\n",
    "# print list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize all vectors\n",
    "\n",
    "Euclidean distance can be a poor metric of similarity between text documents, as it unfairly penalizes long articles. For a reasonable assessment of similarity, we should disregard the length information and use length-agnostic metrics, such as cosine distance. The k-means algorithm does not directly work with cosine distance, so I take an alternative route to remove length information: I normalize all vectors to be unit length. Euclidean distance closely mimics cosine distance when all vectors are unit length. In particular, the squared Euclidean distance between any two vectors of length one is directly proportional to their cosine distance.\n",
    "\n",
    "I use the normalize() function from scikit-learn to normalize all vectors to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Brief overview of k-means\n",
    "\n",
    "The k-means algorithm first chooses an initial set of centroids.\n",
    "\n",
    "After initialization, the k-means algorithm iterates between the following two steps until convergence:\n",
    "1. Assign each data point to the closest centroid.\n",
    "$$\n",
    "z_i \\gets \\mathrm{argmin}_j \\|\\mu_j - \\mathbf{x}_i\\|^2\n",
    "$$\n",
    "2. Revise centroids as the mean of the assigned data points.\n",
    "$$\n",
    "\\mu_j \\gets \\frac{1}{n_j}\\sum_{i:z_i=j} \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "Important decision that we have to make in setting up the algorithm include:\n",
    "<ul>\n",
    "<li> the number of clusters, k\n",
    "<li> the initial set of centroids - k-means converges to a local optimum, and is therefore sensitive to initialization.\n",
    "</ul>\n",
    "\n",
    "## Fit initial k-means model\n",
    "\n",
    "I will start by using k-means++ for initialization and specifying 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 254 ms, sys: 13.3 ms, total: 267 ms\n",
      "Wall time: 223 ms\n",
      "\n",
      "Coordinates of cluster centers: [[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.00378007  0.         ...,  0.          0.00253571  0.        ]\n",
      " [ 0.          0.          0.01231351 ...,  0.0138358   0.          0.00614341]\n",
      " [ 0.07410287  0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.00233756  0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      "Label of each data point\n",
      "[1 1 4 1 1 4 1 2 4 2 2 2 1 4 1 1 1 1 1 1 1 4 1 1 1 4 1 1 1 1 1 1 1 1 2 1 1\n",
      " 4 4 1 1 2 1 2 1 1 4 1 1 1 1 4 1 1 1 1 1 1 1 1 4 2 4 1 1 1 1 4 1 1 1 4 2 1\n",
      " 4 1 2 1 4 4 1 1 1 2 1 1 1 1 2 2 1 4 2 1 1 1 4 1 1 4 1 1 2 2 1 1 1 2 4 1 2\n",
      " 4 1 1 4 4 3 1 4 1 3 1 2 1 2 2 2 1 1 4 1 2 4 1 1 1 1 1 4 4 1 1 1 1 4 0 1 1\n",
      " 4 1 1 1 1 1 1 1 1 4 0 4 2 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 4 1 3 1 1 1 1 4 1\n",
      " 3 3 3 1 4 1 2 1 3 1 2 2 1 1 2]\n",
      "\n",
      "Label of each data point\n",
      "[1 1 4 1 1 4 1 2 4 2 2 2 1 4 1 1 1 1 1 1 1 4 1 1 1 4 1 1 1 1 1 1 1 1 2 1 1\n",
      " 4 4 1 1 2 1 2 1 1 4 1 1 1 1 4 1 1 1 1 1 1 1 1 4 2 4 1 1 1 1 4 1 1 1 4 2 1\n",
      " 4 1 2 1 4 4 1 1 1 2 1 1 1 1 2 2 1 4 2 1 1 1 4 1 1 4 1 1 2 2 1 1 1 2 4 1 2\n",
      " 4 1 1 4 4 3 1 4 1 3 1 2 1 2 2 2 1 1 4 1 2 4 1 1 1 1 1 4 4 1 1 1 1 4 0 1 1\n",
      " 4 1 1 1 1 1 1 1 1 4 0 4 2 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 4 1 3 1 1 1 1 4 1\n",
      " 3 3 3 1 4 1 2 1 3 1 2 2 1 1 2]\n",
      "\n",
      "Sum of distances of samples to their closest cluster center\n",
      "174.641566226\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=5, init='k-means++')\n",
    "%time km = kmeans.fit(tf_idf)\n",
    "print\n",
    "\n",
    "print 'Coordinates of cluster centers:', \n",
    "print km.cluster_centers_\n",
    "print \n",
    "\n",
    "print 'Label of each data point' \n",
    "print km.labels_\n",
    "print \n",
    "\n",
    "print 'Label of each data point' \n",
    "print kmeans.predict(tf_idf) \n",
    "print\n",
    "\n",
    "print 'Sum of distances of samples to their closest cluster center'\n",
    "print kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose k (number of clusters)\n",
    "\n",
    "An important choice we have to make is the number of clusters (the value of k). \n",
    "\n",
    "To compare different valus of k, I need some measure of the performance of different clusterings. One measure I can use is the sum of all squared distances between data points and centroids:\n",
    "$$\n",
    "J(\\mathcal{Z},\\mu) = \\sum_{j=1}^k \\sum_{i:z_i = j} \\|\\mathbf{x}_i - \\mu_j\\|^2.\n",
    "$$\n",
    "\n",
    "This measure makes a lot of sense, since the sum of squared distances between our observations and the cluster centers is the thing that k-means is trying to minimize. I will refer to this measure as cluster heterogenity (the sum of the heterogeneities over all clusters). The larger the distances, the more hetergoenous the clusters are, and the smaller the distances the more homogenous. We would like homogenous/tight clusters. A word of caution with this terminology: the scikit-learn [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score) also uses homogeneity to refer to a particular metric of cluster labeling given the truth.\n",
    "\n",
    "A higher value of k reduces the possible heterogeneity metric by definition, all else equal.  For example, if we have N data points and set k=N clusters, then we could have 0 cluster heterogeneity by setting the N centroids equal to the values of the N data points. One heuristic you can use to choose k is to plot heterogeneity against k and look for the \"elbow\" of the curve. This naturally trades off between trying to minimize heterogeneity, but reduce model complexity.\n",
    "\n",
    "In practice, not all runs for larger k will result in lower heterogenity than a single run with smaller k due to local optima (and thus the importance of initialization). In plotting heterogeneity against k, therefore, for each value of k I will compute heterogeneity for several runs and take the lowest heterogeneity value.\n",
    "\n",
    "Out of curiosity, I also want to take a look at how long it takes to run functions for each \n",
    "value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_heterogeneities(data, kvals, reps):\n",
    "    \"\"\"Returns heterogeneities for a series of k (number of cluster) values\n",
    "    data is a tf-idf matrix\n",
    "    kvals: the k values to use\n",
    "    reps: the number of runs to do for each k value, taking the lowest each time\n",
    "    \"\"\"\n",
    "    het_list = []\n",
    "    comp_times = []\n",
    "    for k in kvals:\n",
    "        start_time = time.time()\n",
    "        sub_het_list = []\n",
    "        for r in range(reps):             \n",
    "            sub_het_list.append(KMeans(n_clusters=k, init='k-means++').fit(tf_idf).inertia_)\n",
    "        het_list.append(min(sub_het_list))\n",
    "        comp_times.append(time.time() - start_time)\n",
    "        if k % 5 == 0:\n",
    "            print k      # to check running ok if takes a while\n",
    "    return kvals, het_list, comp_times\n",
    "\n",
    "f_kvals, f_het_list, f_times = compute_heterogeneities(tf_idf, range(2,100+1), 1)\n",
    "kmeans_het_list = [f\n",
    "with open('kmeans_het_list.txt','w') as myfile:\n",
    "    json.dump(kmeans_het_list, myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('kmeans_het_list.txt','r') as infile:\n",
    "    newList = json.load(infile)\n",
    "\n",
    "def plot_heterogeneity_vs_k(k_values, heterogeneity_values):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(k_values, heterogeneity_values, linewidth=4)\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title('Heterogeneity vs. K')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_runtime_vs_k(k_values, heterogeneity_values):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(k_values, heterogeneity_values, linewidth=4)\n",
    "    plt.xlabel('Runtime')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title('Runtime vs. K')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()    \n",
    "\n",
    "plot_heterogeneity_vs_k(f_kvals, f_het_list)\n",
    "plot_runtime_vs_k(f_kvals, f_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this plot, I select a value of k=XXX.\n",
    "\n",
    "## Review K-means clusters\n",
    "\n",
    "### Examine text in clusters\n",
    "\n",
    "I next want to look at some of the text in my different clusters to see if it seems reasonable. \n",
    "\n",
    "In a good clustering of documents:\n",
    "* Documents in the same cluster should be similar.\n",
    "* Documents from different clusters should be less similar.\n",
    "\n",
    "To examine the text in my clustering I will:\n",
    "* Fetch 5 nearest neighbors of each centroid from the set of documents assigned to that cluster. I will consider these documents as being representative of the cluster.\n",
    "* Print top 5 words that have highest tf-idf weights in each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0    \n",
      "croft 0.492454641646\n",
      "allen 0.492454641646\n",
      "centr 0.327884109405\n",
      "enjoyableeduc 0.246227320823\n",
      "compet 0.246227320823\n",
      "\n",
      "Welcome to Allens Croft Children's Centre At Allens Croft Children's Centre we will give every child the support and opportunity to achieve their potential and celebrate their learning and achievements through enjoyable,educational experiences which \n",
      "\n",
      "Welcome to Allens Croft Children's Centre At Allens Croft Children's Centre we will give every child the support and opportunity to achieve their potential and celebrate their learning and achievements through enjoyable,educational experiences which \n",
      "\n",
      "Welcome to Allens Croft Children's Centre At Allens Croft Children's Centre we will give every child the support and opportunity to achieve their potential and celebrate their learning and achievements through enjoyable,educational experiences which \n",
      "\n",
      "Welcome to Allens Croft Children's Centre At Allens Croft Children's Centre we will give every child the support and opportunity to achieve their potential and celebrate their learning and achievements through enjoyable,educational experiences which \n",
      "\n",
      "Welcome to Adlington St Paul's Church of England Primary School. We are a busy but friendly school, with lots going on. This website aims to give you a taster of the school, as well as providing up to date information for parents, pupils and the wide\n",
      "\n",
      "==========================================================\n",
      "Cluster 1    \n",
      "school 0.140205584799\n",
      "welcom 0.0588667073451\n",
      "websit 0.0520779476816\n",
      "abbey 0.0460788789938\n",
      "inform 0.0416553054774\n",
      "\n",
      "Welcome to the Federation of Abbey Schools website. We hope you will find it useful and informative. We believe our school is a special place where everyone feels part of a caring, learning environment. We are very proud of all we achieve. The pupils\n",
      "\n",
      "Welcome to the Federation of Abbey Schools website. We hope you will find it useful and informative. We believe our school is a special place where everyone feels part of a caring, learning environment. We are very proud of all we achieve. The pupils\n",
      "\n",
      "Welcome to the Federation of Abbey Schools website. We hope you will find it useful and informative. We believe our school is a special place where everyone feels part of a caring, learning environment. We are very proud of all we achieve. The pupils\n",
      "\n",
      "Welcome to the Federation of Abbey Schools website. We hope you will find it useful and informative. We believe our school is a special place where everyone feels part of a caring, learning environment. We are very proud of all we achieve. The pupils\n",
      "\n",
      "Login Parents Headteacher's Message Parents Headteacher's Message Welcome to Alnwick The Duke's Middle School I have great pleasure in introducing our school website and I hope that you will find the information contained in it useful and helpful. We\n",
      "\n",
      "==========================================================\n",
      "Cluster 2    \n",
      "academi 0.182374624239\n",
      "princip 0.173169849612\n",
      "student 0.106390352619\n",
      "trust 0.0533718083947\n",
      "educ 0.0442809052504\n",
      "\n",
      "Horizons Horizons Specialist Academy Trust I am delighted to welcome you to the website of Horizons Specialist Academy Trust (HSAT) and to share with you the wide range of learning opportunities available to our pupils and students. Horizons Speciali\n",
      "\n",
      "Horizons Horizons Specialist Academy Trust I am delighted to welcome you to the website of Horizons Specialist Academy Trust (HSAT) and to share with you the wide range of learning opportunities available to our pupils and students. Horizons Speciali\n",
      "\n",
      "Principal's Welcome Welcome to All Saints' Academy At All Saints' Academy there is a real desire to achieve excellence in all that we do whilst living out our Christian values of love, peace, justice, respect, reconciliation and service to others. Ou\n",
      "\n",
      "Welcome from the Principal I am extremely proud to be the Principal of the Academy and I will work alongside the students to create an environment that encourages every member of our community to live well together. It is important to us that student\n",
      "\n",
      "Welcome to Admirals Academy Sponsored by Academy Transformation Trust We pride ourselves on our varied and rich curriculum with a commitment to uncovering the hidden potential which we believe is in every child. Our vision is to develop a 21st centur\n",
      "\n",
      "==========================================================\n",
      "Cluster 3    \n",
      "school 0.142060754819\n",
      "children 0.0984083516819\n",
      "learn 0.0529632096605\n",
      "primari 0.0527610146608\n",
      "communiti 0.0498712127562\n",
      "\n",
      "Welcome to All Saints Anglican/Methodist Primary School On behalf of our children, staff and governors I would like to extend a very warm welcome to the All Saints Primary School family. We are a small village school, based in the heart of the beauti\n",
      "\n",
      "Welcome On behalf of the children, staff and governors Id like to welcome you to All Saints CE Primary School. The school is a member of The Ark Federation and works in partnership with Beer CofE Primary School. Our Federation is also part of St Chri\n",
      "\n",
      "Welcome to Ashleigh Primary School and Nurserys website. The purpose of this website is to provide an insight into the high quality provision offered within our school. We hope it will be of interest to existing members of our community and for those\n",
      "\n",
      "Archibald Primary School A Welcome to Archibald Primary School Website from the Headteacher Dear Parents and Carers, Thank you for taking the time to visit our website. We hope you find it a useful source of information and that it gives a good insig\n",
      "\n",
      "Welcome Welcome to Bardney Church of England and Methodist Primary School On behalf of the children, staff and governors I wish you a very warm welcome. At Bardney Church of England and Methodist Primary School we believe our role is to ensure that a\n",
      "\n",
      "==========================================================\n",
      "Cluster 4    \n",
      "river 0.266189723678\n",
      "chorleywood 0.266189723678\n",
      "rickmansworth 0.266189723678\n",
      "croxley 0.266189723678\n",
      "loudwat 0.266189723678\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "Welcome to Three Rivers West Children's Centres We offer access to a range of services and advice to families with young children living in the Chorleywood & Loudwater, Croxley Green and Rickmansworth area. Find out more\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=5, init='k-means++', random_state=0).fit(tf_idf)  # set seed for consistent results\n",
    "centroids = km.cluster_centers_   # coordinates of cluster centers\n",
    "cluster_assignment = km.labels_   # label of every data point\n",
    "n_clusters = 5\n",
    "\n",
    "for c in xrange(n_clusters):\n",
    "    # Cluster heading\n",
    "    print('Cluster {0:d}    '.format(c))\n",
    "\n",
    "    # Print top 5 words with largest TF-IDF weights in the cluster\n",
    "    idx = centroids[c].argsort()[::-1]\n",
    "    for i in xrange(5):\n",
    "        print list_of_words[idx[i]], centroids[c,idx[i]]\n",
    "    print ('')\n",
    "    \n",
    "    # Compute distances from the centroid to all data points in the cluster\n",
    "    distances = pairwise_distances(tf_idf, [centroids[c]], metric='euclidean').flatten()\n",
    "    distances[cluster_assignment!=c] = float('inf') # remove non-members from consideration\n",
    "    nearest_neighbors = distances.argsort() # argsort() returns the indices that would sort an array\n",
    "    # For 5 nearest neighbors, print the first 250 characters of text\n",
    "    for i in xrange(5):\n",
    "        print blist[nearest_neighbors[i]][:250]\n",
    "        print ('')\n",
    "    print('==========================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe of URNs, blurbs, and their clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URN</th>\n",
       "      <th>blurbs</th>\n",
       "      <th>km_assignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119460.0</td>\n",
       "      <td>Welcome to Adlington St Paul's Church of Engla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139709.0</td>\n",
       "      <td>Welcome to Abbey Woods Academy Abbey Woods is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114864.0</td>\n",
       "      <td>We aim to make school a happy and rewarding ex...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131982.0</td>\n",
       "      <td>Menu Welcome from the Principal Principals Blo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121326.0</td>\n",
       "      <td>Welcome to Alanbrooke Community Primary School...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        URN                                             blurbs  km_assignment\n",
       "0  119460.0  Welcome to Adlington St Paul's Church of Engla...              1\n",
       "1  139709.0  Welcome to Abbey Woods Academy Abbey Woods is ...              1\n",
       "2  114864.0  We aim to make school a happy and rewarding ex...              3\n",
       "3  131982.0  Menu Welcome from the Principal Principals Blo...              2\n",
       "4  121326.0  Welcome to Alanbrooke Community Primary School...              3"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifed_blurbs = pd.DataFrame(\n",
    "    {'URN': urn_list,\n",
    "     'blurbs': blist,\n",
    "     'km_assignment': cluster_assignment\n",
    "    })\n",
    "classifed_blurbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize clusters\n",
    "\n",
    "#### Dimensionality reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
